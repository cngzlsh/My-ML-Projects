{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import sacrebleu\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce MX150\n"
     ]
    }
   ],
   "source": [
    "# train on GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name())\n",
    "'''\n",
    "Note:\n",
    "assert len(train_iterator) % batch_size == 0\n",
    "or notebook will return tensor dimension mismatch when training the last batch\n",
    "in this notebook len(train_iterator) == 29000, so there will be 580 batches\n",
    "'''\n",
    "batch_size = 50 # I tried several batch sizes, this does not return 'CUDA out of memory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:20: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['der', 'mann', 'trinkt', 'bier']\n",
      "Training set length: 29000\n",
      "Number of training batches: 580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-0d62185da844>:20: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(train_iterator) % batch_size == 0, 'Possible tensor dimension mismatch error while training, consider changing batch size.')\n"
     ]
    }
   ],
   "source": [
    "# loading data and word preprocessing using spaCy\n",
    "german_text = Field(tokenize = \"spacy\", \n",
    "                    tokenizer_language=\"de\", \n",
    "                    init_token = '<sos>', \n",
    "                    eos_token = '<eos>', \n",
    "                    lower = True)\n",
    "english_text = Field(tokenize = \"spacy\", \n",
    "                     tokenizer_language=\"en\", \n",
    "                     init_token = '<sos>', \n",
    "                     eos_token = '<eos>', \n",
    "                     lower = True)\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (german_text, english_text))\n",
    "print(german_text.preprocess('der mann trinkt bier'))\n",
    "print(f\"Training set length: {len(train_data.examples)}\")\n",
    "german_text.build_vocab(train_data, min_freq = 4)\n",
    "english_text.build_vocab(train_data, min_freq = 4)\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size = batch_size, device = device)\n",
    "print(f'Number of training batches: {len(train_iterator)}')\n",
    "assert(len(train_iterator) % batch_size == 0, 'Possible tensor dimension mismatch error while training, consider changing batch size.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German example: eine blondine hält mit einem mann im sand händchen .\n",
      "English example: a blond holding hands with a guy in the sand .\n"
     ]
    }
   ],
   "source": [
    "# a random example in the test set\n",
    "random_index = random.randint(0, len(test_data.examples))\n",
    "ger_example = vars(test_data.examples[random_index])['src']\n",
    "eng_example = vars(test_data.examples[random_index])['trg']\n",
    "print(\"German example:\", \" \".join(ger_example))\n",
    "print(\"English example:\", \" \".join(eng_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "class German_Encoder(nn.Module):\n",
    "    def __init__(self, dimensions, layers):\n",
    "        super(German_Encoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = dimensions['input']\n",
    "        self.embedding_dim = dimensions['embedding']\n",
    "        self.hidden_dim = dimensions['hidden']\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_dim, self.embedding_dim, padding_idx=1)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, layers, dropout=0.25, bias=False, batch_first=True, bidirectional=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim*2, self.hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x.permute(1, 0))\n",
    "        x = self.dropout(x)\n",
    "        outputs, (hidden_state, cell_state) = self.lstm(x)\n",
    "        forward_hidden, backward_hidden = hidden_state[0::2,:,:], hidden_state[1::2,:,:]\n",
    "        forward_cell, backward_cell = cell_state[0::2,:,:], cell_state[1::2,:,:]\n",
    "        \n",
    "        h_n = torch.cat((forward_hidden, backward_hidden), dim=-1)\n",
    "        c_n = torch.cat((forward_cell, backward_cell), dim=-1)\n",
    "        h_n, c_n = self.ff(h_n), self.ff(c_n)\n",
    "        return outputs, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention model\n",
    "class Calculate_Attention(nn.Module):\n",
    "    def __init__(self, dimensions):\n",
    "        super(Calculate_Attention, self).__init__()\n",
    "        self.hidden_dim = dimensions['hidden']\n",
    "        \n",
    "        self.W = nn.Linear(3 * self.hidden_dim, self.hidden_dim)\n",
    "        self.v = nn.Linear(self.hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, dec_hidden, enc_outputs):\n",
    "#         print(dec_hidden.shape)\n",
    "#         print(enc_outputs.shape)\n",
    "        dec_hidden = dec_hidden.permute(1, 0, 2)\n",
    "        dec_hidden = dec_hidden.repeat(1, enc_outputs.shape[1], 1)\n",
    "        concat = torch.cat((dec_hidden, enc_outputs), dim=2)\n",
    "        energy = torch.tanh(self.W(concat))\n",
    "        \n",
    "        attention = F.softmax(self.v(energy).squeeze(2), dim=1)\n",
    "        \n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "class English_Decoder(nn.Module):\n",
    "    def __init__(self, dimensions, layers):\n",
    "        super(English_Decoder, self).__init__()\n",
    "        \n",
    "        self.output_dim = dimensions['output']\n",
    "        self.embedding_dim = dimensions['embedding']\n",
    "        self.hidden_dim = dimensions['hidden']\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.embedding = nn.Embedding(self.output_dim, self.embedding_dim, padding_idx=1)\n",
    "        self.attention = Calculate_Attention(dimensions)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim + 2*self.hidden_dim, self.hidden_dim, self.layers, dropout=0.1, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(3*self.hidden_dim + self.embedding_dim, self.embedding_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.embedding_dim, self.output_dim)\n",
    "        \n",
    "    def forward(self, x, hidden, enc_outputs):\n",
    "        x = self.embedding(x.unsqueeze(1))\n",
    "        x_embed = self.dropout(x)\n",
    "        attention_score = self.attention(hidden[0], enc_outputs).unsqueeze(1)\n",
    "        context = torch.bmm(attention_score, enc_outputs)\n",
    "        \n",
    "        rnn_inputs = torch.cat((x_embed, context), dim=2)\n",
    "        output, hidden = self.lstm(rnn_inputs, hidden)\n",
    "        output = self.ff(torch.cat((output, context, x_embed), dim=2))\n",
    "        output = self.output_layer(output).squeeze(1)\n",
    "        attention_return = attention_score.squeeze(1)\n",
    "        return output, hidden, attention_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq model\n",
    "class Machine_Translation_Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Machine_Translation_Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, german, english, learning=True):\n",
    "        sentence_len = english.shape[0]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(sentence_len, batch_size, output_dim).to(device)\n",
    "        \n",
    "        enc_outputs, hidden_states = self.encoder(german)\n",
    "        \n",
    "        dec_output = english[0,:]\n",
    "        for t in range(1, sentence_len):\n",
    "            dec_output, hidden, _ = self.decoder(dec_output, hidden_states, enc_outputs)\n",
    "\n",
    "            outputs[t] = dec_output\n",
    "            \n",
    "            pred_next = dec_output.argmax(1)\n",
    "            \n",
    "            if learning and random.random() < 0.5:\n",
    "                dec_output = english[t]\n",
    "            else:\n",
    "                dec_output = pred_next\n",
    "        return outputs\n",
    "    \n",
    "    def translate(self, input_tensor):\n",
    "        input_tensor.to(device)\n",
    "        \n",
    "        outputs = torch.zeros(50, input_tensor.shape[1]).to(device)\n",
    "        attentions = torch.zeros(50, input_tensor.shape[1], input_tensor.shape[0]).to(device)\n",
    "        \n",
    "        enc_outputs, hidden_states = self.encoder(input_tensor)\n",
    "        \n",
    "        dec_output = torch.zeros(input_tensor.shape[1], dtype=torch.int64).to(device)\n",
    "        dec_output.fill_(english_text.vocab.stoi['<sos>'])\n",
    "        outputs[0] = dec_output\n",
    "        \n",
    "        for t in range(1, 50):\n",
    "            dec_output, hidden, attention_score = self.decoder(dec_output, hidden_states, enc_outputs)\n",
    "            attentions[t] = attention_score\n",
    "            dec_output = dec_output.argmax(1)\n",
    "            outputs[t] = dec_output\n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(model, iterator, optimiser, criterion, epochs, force_stop=False):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for i in range(epochs): \n",
    "        print(f'Start training epoch {i+1} of {epochs}.')\n",
    "        epoch_loss = 0\n",
    "        for j, batch in tqdm(enumerate(iterator)):\n",
    "            german = batch.src.to(device)\n",
    "            english = batch.trg.to(device)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(german, english)\n",
    "        \n",
    "            outputs = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            english = english[1:].view(-1)\n",
    "\n",
    "            loss = criterion(outputs, english)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimiser.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            if force_stop and j == 20:\n",
    "                break\n",
    "        elapsed_time = int(time.time() - start)\n",
    "        elapsed_mins = int(elapsed_time/60)\n",
    "        elapsed_secs = int(elapsed_time - elapsed_mins * 60)\n",
    "        losses.append(epoch_loss)\n",
    "        print(f'Epoch {i+1} of {epochs} complete. Epoch loss: {epoch_loss}. Time elapsed: {elapsed_mins} minutes, {elapsed_secs} seconds')\n",
    "    end = time.time()\n",
    "    total_duration = end - start\n",
    "    total_mins = int(total_duration/60)\n",
    "    total_secs = int(total_duration - 60 * total_mins)\n",
    "    print(f'Training complete. Time elapsed: {total_mins} minutes, {total_secs} seconds')\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    xaxis = [i+1 for i in range(epochs)]\n",
    "    plt.plot(xaxis, losses)\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cngzl\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\cngzl\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "dimensions = {'input': len(german_text.vocab),\n",
    "             'output': len(english_text.vocab),\n",
    "             'embedding': 256,\n",
    "             'hidden': 512}\n",
    "layers = 1\n",
    "epochs = 50\n",
    "\n",
    "enc = German_Encoder(dimensions, layers).to(device)\n",
    "dec = English_Decoder(dimensions, layers).to(device)\n",
    "model = Machine_Translation_Model(enc, dec).to(device)\n",
    "\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training epoch 1 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:04,  1.59it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50 complete. Epoch loss: 2623.0778691768646. Time elapsed: 6 minutes, 4 seconds\n",
      "Start training epoch 2 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:03,  1.59it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of 50 complete. Epoch loss: 2168.2279255390167. Time elapsed: 12 minutes, 8 seconds\n",
      "Start training epoch 3 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:05,  1.58it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 50 complete. Epoch loss: 1965.3506200313568. Time elapsed: 18 minutes, 14 seconds\n",
      "Start training epoch 4 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:06,  1.58it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 of 50 complete. Epoch loss: 1824.9099490642548. Time elapsed: 24 minutes, 21 seconds\n",
      "Start training epoch 5 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:08,  1.57it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 50 complete. Epoch loss: 1722.2633941173553. Time elapsed: 30 minutes, 30 seconds\n",
      "Start training epoch 6 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:11,  1.56it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 of 50 complete. Epoch loss: 1636.9639930725098. Time elapsed: 36 minutes, 41 seconds\n",
      "Start training epoch 7 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:09,  1.57it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 of 50 complete. Epoch loss: 1564.3674008846283. Time elapsed: 42 minutes, 51 seconds\n",
      "Start training epoch 8 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:12,  1.56it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 of 50 complete. Epoch loss: 1497.1907048225403. Time elapsed: 49 minutes, 3 seconds\n",
      "Start training epoch 9 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:12,  1.56it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 of 50 complete. Epoch loss: 1434.6117143630981. Time elapsed: 55 minutes, 15 seconds\n",
      "Start training epoch 10 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:12,  1.56it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 of 50 complete. Epoch loss: 1385.628167271614. Time elapsed: 61 minutes, 28 seconds\n",
      "Start training epoch 11 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:13,  1.55it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 of 50 complete. Epoch loss: 1348.753401517868. Time elapsed: 67 minutes, 41 seconds\n",
      "Start training epoch 12 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:27,  1.50it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 of 50 complete. Epoch loss: 1300.9247907400131. Time elapsed: 74 minutes, 9 seconds\n",
      "Start training epoch 13 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:24,  1.51it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 of 50 complete. Epoch loss: 1272.3911609649658. Time elapsed: 80 minutes, 33 seconds\n",
      "Start training epoch 14 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:24,  1.51it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 of 50 complete. Epoch loss: 1233.1801789999008. Time elapsed: 86 minutes, 58 seconds\n",
      "Start training epoch 15 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:16,  1.54it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 of 50 complete. Epoch loss: 1199.919527053833. Time elapsed: 93 minutes, 15 seconds\n",
      "Start training epoch 16 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:19,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 of 50 complete. Epoch loss: 1185.0555554628372. Time elapsed: 99 minutes, 34 seconds\n",
      "Start training epoch 17 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:18,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 of 50 complete. Epoch loss: 1164.1193755865097. Time elapsed: 105 minutes, 53 seconds\n",
      "Start training epoch 18 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:19,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 of 50 complete. Epoch loss: 1146.5450068712234. Time elapsed: 112 minutes, 12 seconds\n",
      "Start training epoch 19 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:20,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 of 50 complete. Epoch loss: 1115.5808767080307. Time elapsed: 118 minutes, 33 seconds\n",
      "Start training epoch 20 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:18,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 of 50 complete. Epoch loss: 1087.9848088026047. Time elapsed: 124 minutes, 51 seconds\n",
      "Start training epoch 21 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:18,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 of 50 complete. Epoch loss: 1076.9182275533676. Time elapsed: 131 minutes, 9 seconds\n",
      "Start training epoch 22 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:17,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 of 50 complete. Epoch loss: 1051.1438019275665. Time elapsed: 137 minutes, 27 seconds\n",
      "Start training epoch 23 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:19,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 of 50 complete. Epoch loss: 1047.5525796413422. Time elapsed: 143 minutes, 47 seconds\n",
      "Start training epoch 24 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:16,  1.54it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 of 50 complete. Epoch loss: 1032.5792971849442. Time elapsed: 150 minutes, 4 seconds\n",
      "Start training epoch 25 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:17,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 50 complete. Epoch loss: 1011.8877815008163. Time elapsed: 156 minutes, 22 seconds\n",
      "Start training epoch 26 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:21,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 of 50 complete. Epoch loss: 1001.7963573932648. Time elapsed: 162 minutes, 43 seconds\n",
      "Start training epoch 27 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:19,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 of 50 complete. Epoch loss: 989.7415992021561. Time elapsed: 169 minutes, 3 seconds\n",
      "Start training epoch 28 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:16,  1.54it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 of 50 complete. Epoch loss: 988.2319512367249. Time elapsed: 175 minutes, 19 seconds\n",
      "Start training epoch 29 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:18,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 of 50 complete. Epoch loss: 971.7747563123703. Time elapsed: 181 minutes, 37 seconds\n",
      "Start training epoch 30 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:20,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 of 50 complete. Epoch loss: 967.7264209985733. Time elapsed: 187 minutes, 57 seconds\n",
      "Start training epoch 31 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:20,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 of 50 complete. Epoch loss: 945.97550791502. Time elapsed: 194 minutes, 17 seconds\n",
      "Start training epoch 32 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:22,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 of 50 complete. Epoch loss: 938.5555775165558. Time elapsed: 200 minutes, 40 seconds\n",
      "Start training epoch 33 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:19,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 of 50 complete. Epoch loss: 947.0901989936829. Time elapsed: 206 minutes, 59 seconds\n",
      "Start training epoch 34 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:20,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 of 50 complete. Epoch loss: 929.0326473712921. Time elapsed: 213 minutes, 20 seconds\n",
      "Start training epoch 35 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:19,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 of 50 complete. Epoch loss: 912.025520503521. Time elapsed: 219 minutes, 40 seconds\n",
      "Start training epoch 36 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:20,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 of 50 complete. Epoch loss: 937.3617724180222. Time elapsed: 226 minutes, 0 seconds\n",
      "Start training epoch 37 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:18,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 of 50 complete. Epoch loss: 895.8426998853683. Time elapsed: 232 minutes, 19 seconds\n",
      "Start training epoch 38 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:20,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 of 50 complete. Epoch loss: 907.1809360384941. Time elapsed: 238 minutes, 40 seconds\n",
      "Start training epoch 39 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:19,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 of 50 complete. Epoch loss: 893.3952022790909. Time elapsed: 244 minutes, 59 seconds\n",
      "Start training epoch 40 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:20,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 of 50 complete. Epoch loss: 886.6013377904892. Time elapsed: 251 minutes, 19 seconds\n",
      "Start training epoch 41 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:23,  1.51it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 of 50 complete. Epoch loss: 867.0150805711746. Time elapsed: 257 minutes, 43 seconds\n",
      "Start training epoch 42 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:19,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 of 50 complete. Epoch loss: 878.6416050195694. Time elapsed: 264 minutes, 2 seconds\n",
      "Start training epoch 43 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:19,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 of 50 complete. Epoch loss: 863.6666631698608. Time elapsed: 270 minutes, 22 seconds\n",
      "Start training epoch 44 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:17,  1.54it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 of 50 complete. Epoch loss: 866.792387843132. Time elapsed: 276 minutes, 40 seconds\n",
      "Start training epoch 45 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:20,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 of 50 complete. Epoch loss: 863.5503120422363. Time elapsed: 283 minutes, 0 seconds\n",
      "Start training epoch 46 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:21,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 of 50 complete. Epoch loss: 849.7544066309929. Time elapsed: 289 minutes, 22 seconds\n",
      "Start training epoch 47 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:20,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 of 50 complete. Epoch loss: 851.3227222561836. Time elapsed: 295 minutes, 43 seconds\n",
      "Start training epoch 48 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:19,  1.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 of 50 complete. Epoch loss: 843.2699930667877. Time elapsed: 302 minutes, 2 seconds\n",
      "Start training epoch 49 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:22,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 of 50 complete. Epoch loss: 840.1098526120186. Time elapsed: 308 minutes, 24 seconds\n",
      "Start training epoch 50 of 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [06:18,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 of 50 complete. Epoch loss: 841.1988242864609. Time elapsed: 314 minutes, 43 seconds\n",
      "Training complete. Time elapsed: 314 minutes, 43 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6sAAAHiCAYAAAAOKloIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeXxV1b3+8eebk4nMDEkgCTMHEJAxKjhRZ5yKgrYO1VZrrb1aq3a+9/Zn7+1we1urt1arda51rmClVStOBQVRwqyMAQIkhBAShkDIvH5/nI09aoCEDPvk5PN+vc6Lw9r77PMc+k8f99prmXNOAAAAAABEkhi/AwAAAAAA8FmUVQAAAABAxKGsAgAAAAAiDmUVAAAAABBxKKsAAAAAgIhDWQUAAAAARBzKKgAAaBEzc2Y2zO8cAIDugbIKAOjyzKzIzM72OwcAAGg/lFUAACKcmQX8zgAAQGejrAIAopqZfcPMCs2s0szmmFmON25mdo+Z7TSzvWa20szGeMcuMLPVZlZlZiVm9r3DXDvGzP7TzLZ413nSzNK9Y/8ws1s+c/4KM5vhvR9pZm94udaZ2ZfCznvCzB4ws1fN7ICkM5r57nQze9TMSr2MPz9Uas3sa2a2wMx+7/22tWZ2Vthnc7x/i0rv3+YbYccCZvbvZrbR+/1LzKx/2FefbWYbzGy3md1vZuZ9bpiZzfO+b5eZPd/a/60AAAhHWQUARC0zO1PS/0j6kqR+krZIes47fK6k0yUNl5Qh6cuSKrxjj0r6pnMuVdIYSW8f5iu+5r3OkDREUoqk+7xjz0i6MizLKEkDJb1iZsmS3vDOyfLO+4OZjQ679lWSfiEpVdJ7zXz3nyQ1SBomaYL3e24IO36SpE2S+ki6U9JsM+vlHXtWUrGkHEmXSfplWJm9w8tzgaQ0SddLqg677kWSTpA0TqF/1/O88Z9Jmiupp6Q8Sb9vJjMAAC1GWQUARLOrJT3mnFvqnKuV9GNJU8xskKR6hYrgSEnmnFvjnCv1PlcvaZSZpTnndjvnlh7h+nc75zY55/Z717/CzGIlvSRpvJkNDDt3tpfjIklFzrnHnXMN3vVnKVQcD3nZObfAOdfknKsJ/1Izy5Z0vqTbnHMHnHM7Jd0j6Yqw03ZK+j/nXL1z7nlJ6yRd6N0lPVXSD51zNc655ZIekXSN97kbJP2nc26dC1nhnKsIu+6vnHN7nHNbJb0jaXzYv9lASTnedZsr2AAAtBhlFQAQzXIUupsqSfIKZYWkXOfc2wrdBb1fUpmZPWRmad6pMxW6s7jFm9o6pSXX997HSsp2zlVJekX/KpBXSHraez9Q0klmtufQS6Ey2zfsWtuO8LsGSoqTVBr2+T8qdJf2kBLnnPtMthzvVenlCz+W673vL2njEb57R9j7aoXuJkvSDySZpA/N7GMzu/4I1wAA4KgoqwCAaLZdoWInSfKm3/aWVCJJzrl7nXOTJI1WaDrw973xxc656QqVv79KeqEl15c0QKGpuWXe35+VdKVXdnsodCdSChXRec65jLBXinPuW2HXCi+an7VNUq2kPmGfT3POhU8jzj30PGlYtu3eq5eZpX7mWEnYtYce4bub5Zzb4Zz7hnMuR9I3FZrWzDY3AIBjRlkFAESLODNLDHvFKvRM6HVmNt7MEiT9UtIHzrkiMzvBzE4yszhJByTVSGo0s3gzu9rM0p1z9ZL2SWo8zHc+K+l2MxtsZine9Z93zjV4x19VqMz+tzfe5I3/XdJwM7vGzOK81wlmdlxLfqg3XXmupN+aWZq30NNQM5sadlqWpFu9a18u6ThJrzrntklaKOl/vH+nsZK+rn/d9X1E0s/MLBhag8rGmlnvo2Uys8vNLM/7626Fyvbh/t0AADgqyioAIFq8Kulg2Ounzrm3JP1EoedBSxW6Y3hoWm6apIcVKlZbFJoefJd37BpJRWa2T9JNkr5ymO98TNKfJc2XtFmhwvvtQwe951NnSzpboeJ8aLxKoQWRrlDoTucOSf8rKaEVv/daSfGSVnu/4UWFFpE65ANJQUm7FFqo6bKwZ0+vlDTI++6XJN3pnHvDO3a3QneS5ypU1B9V6K7w0Zwg6QMz2y9pjqTvOOc2t+L3AADwKfbpx1kAAEBXZ2Zfk3SDc+5Uv7MAAHCsuLMKAAAAAIg4lFUAAAAAQMRhGjAAAAAAIOJwZxUAAAAAEHEoqwAAAACAiBPrd4Cj6dOnjxs0aJDfMQAAAAAA7WzJkiW7nHOZzR2L+LI6aNAgFRQU+B0DAAAAANDOzGzL4Y4xDRgAAAAAEHEoqwAAAACAiENZBQAAAABEHMoqAAAAACDiUFYBAAAAABGHsgoAAAAAiDiUVQAAAABAxKGsAgAAAAAiDmUVAAAAABBxKKsAAAAAgIhDWQUAAAAARBzKKgAAAAAg4lBWAQAAAAARh7IKAAAAAIg4lFUAAAAAQMShrAIAAAAAIg5ltQ2ampzKq2r9jgEAAAAAUYey2gY/mLVS0+97z+8YAAAAABB1KKttMCQzWdv31qiqpt7vKAAAAAAQVSirbRDMSpUkbSw/4HMSAAAAAIgulNU2CGalSJI2lFX5nAQAAAAAogtltQ3690pSfGyMCnfu9zsKAAAAAEQVymobBGJMQzNTtIGyCgAAAADtirLaRsGsFG3YyTRgAAAAAGhPlNU2CmalqHj3QVXXNfgdBQAAAACiBmW1jYLZKXJO2sSKwAAAAADQbiirbTTM276GqcAAAAAA0H4oq200sHeS4gKmDWUssgQAAAAA7YWy2kZxgRgN7pPMisAAAAAA0I4oq+0gmJWqDWVMAwYAAACA9kJZbQfDslK0tbJaNfWNfkcBAAAAgKhAWW0HwewUNbEiMAAAAAC0G8pqOwiyIjAAAAAAtCvKajsY1CdJgRhTIYssAQAAAEC7oKy2g4TYgAb2TmL7GgAAAABoJ5TVdhLMSmEaMAAAAAC0E8pqOwlmpaqoolp1DU1+RwEAAACALo+y2k6C2SlqbHIqqmBFYAAAAABoK8pqOxmWlSJJPLcKAAAAAO2AstpOhmamyIztawAAAACgPVBW20liXEADeiVpA9vXAAAAAECbHbWsmll/M3vHzNaY2cdm9h1v/KdmVmJmy73XBWGf+bGZFZrZOjM7L2x8mjdWaGY/6pif5J9gVqoKmQYMAAAAAG0W24JzGiR91zm31MxSJS0xsze8Y/c45+4KP9nMRkm6QtJoSTmS3jSz4d7h+yWdI6lY0mIzm+OcW90ePyQSBLNTNG/9TjU0Nik2wE1rAAAAADhWR21UzrlS59xS732VpDWSco/wkemSnnPO1TrnNksqlHSi9yp0zm1yztVJes47N2oEs1JU3+i0pbLa7ygAAAAA0KW16vafmQ2SNEHSB97QLWa20sweM7Oe3liupG1hHyv2xg43HjWCWamSWBEYAAAAANqqxWXVzFIkzZJ0m3Nun6QHJA2VNF5SqaTfHjq1mY+7I4w39103mlmBmRWUl5e3NKLvhmYlS5IKWREYAAAAANqkRWXVzOIUKqpPO+dmS5Jzrsw51+ica5L0sELTfKXQHdP+YR/Pk7T9COOf45x7yDmX75zLz8zMbM3v8VVSfKzyevZgRWAAAAAAaKOWrAZskh6VtMY5d3fYeL+w0y6V9JH3fo6kK8wswcwGSwpK+lDSYklBMxtsZvEKLcI0p31+RuQIZqUwDRgAAAAA2qglqwGfIukaSavMbLk39u+SrjSz8QpN5S2S9E1Jcs59bGYvSFqt0ErCNzvnGiXJzG6R9LqkgKTHnHMft+NviQjB7FQt3FihxianQExzM58BAAAAAEdz1LLqnHtPzT9v+uoRPvMLSb9oZvzVI30uGgzLSlFtQ5OKd1drYO9kv+MAAAAAQJfEZqDtLJiVIklaz1RgAAAAADhmlNV2NswrqxtYERgAAAAAjhlltZ2lJsapX3qiCrmzCgAAAADHjLLaAYZlpbB9DQAAAAC0AWW1AwSzUlW4c7+ampzfUQAAAACgS6KsdoBgdooO1jeqZM9Bv6MAAAAAQJdEWe0Ah1YELmQqMAAAAAAcE8pqB2BFYAAAAABoG8pqB8hIildmaoI2sCIwAAAAABwTymoHCbIiMAAAAAAcM8pqBwlmpahw5345x4rAAAAAANBalNUOMiw7VftrG7RjX43fUQAAAACgy6GsdpBDKwLz3CoAAAAAtB5ltYN8UlZ5bhUAAAAAWo2y2kF6pySoV3K8Ctm+BgAAAABajbLagYZlpTANGAAAAACOAWW1Aw3PDm1fw4rAAAAAANA6lNUOFMxK1d6D9SrfX+t3FAAAAADoUiirHejQIkuFTAUGAAAAgFahrHagYdmsCAwAAAAAx4Ky2oEyUxKU3iNOG1gRGAAAAABahbLagcxMwawUrWcaMAAAAAC0CmW1gwWzU1TINGAAAAAAaBXKagcblpWqygN1qmBFYAAAAABoMcpqBzu0IjCLLAEAAABAy1FWO1iQFYEBAAAAoNUoqx2sb1qiUhJiVVjGisAAAAAA0FKU1Q5mZhqWlcKdVQAAAABoBcpqJwhSVgEAAACgVSirnSCYnaLyqlrtqa7zOwoAAAAAdAmU1U4QzEqVJPZbBQAAAIAWoqx2gmFsXwMAAAAArUJZ7QS5GT3UIy6gDWWUVQAAAABoCcpqJ4iJObQiMNvXAAAAAEBLUFY7STArhWdWAQAAAKCFKKudZFh2ikr31qiqpt7vKAAAAAAQ8SirnYQVgQEAAACg5SirnSTIisAAAAAA0GKU1U7Sv1eS4mNjuLMKAAAAAC1AWe0kgRjT0MwUbShjRWAAAAAAOBrKaicKZqUwDRgAAAAAWoCy2omGZ6eoePdBVdc1+B0FAAAAACIaZbUTDWNFYAAAAABoEcpqJwpmeysCl1FWAQAAAOBIKKudaGCvJMUFjOdWAQAAAOAoKKudKDYQoyF9UlS4kxWBAQAAAOBIKKudbFg2KwIDAAAAwNEctayaWX8ze8fM1pjZx2b2HW/8N2a21sxWmtlLZpbhjQ8ys4Nmttx7PRh2rUlmtsrMCs3sXjOzjvtpkSmYlaKtldWqqW/0OwoAAAAARKyW3FltkPRd59xxkiZLutnMRkl6Q9IY59xYSesl/TjsMxudc+O9101h4w9IulFS0HtNa48f0ZUEs1LlnLSxnLurAAAAAHA4Ry2rzrlS59xS732VpDWScp1zc51zhzYMXSQp70jXMbN+ktKcc+8755ykJyVd0qb0XdChFYHZvgYAAAAADq9Vz6ya2SBJEyR98JlD10t6Lezvg81smZnNM7PTvLFcScVh5xR7Y819z41mVmBmBeXl5a2JGPEG9U5WIMbYvgYAAAAAjqDFZdXMUiTNknSbc25f2Ph/KDRV+GlvqFTSAOfcBEl3SHrGzNIkNfd8qmvuu5xzDznn8p1z+ZmZmS2N2CXEx8ZoUO8kbWBFYAAAAAA4rNiWnGRmcQoV1aedc7PDxr8q6SJJZ3lTe+Wcq5VU671fYmYbJQ1X6E5q+FThPEnb2+NHdDXBrFStp6wCAAAAwGG1ZDVgk/SopDXOubvDxqdJ+qGkLzrnqsPGM80s4L0fotBCSpucc6WSqsxssnfNayW93K6/posIZqdoS0W1ahtYERgAAAAAmtOSacCnSLpG0plh29FcIOk+SamS3vjMFjWnS1ppZiskvSjpJudcpXfsW5IekVQoaaM+/ZxrtzEsK0WNTU5Fu6qPfjIAAAAAdENHnQbsnHtPzT9v+uphzp+l0JTh5o4VSBrTmoDRKJiVKknasLNKI/qm+pwGAAAAACJPq1YDRvsYkpmsGBMrAgMAAADAYVBWfZAYF9CAXknstQoAAAAAh0FZ9cmwrFS2rwEAAACAw6Cs+iSYnaLNuw6ovrHJ7ygAAAAAEHEoqz4JZqWovtFpSwUrAgMAAADAZ1FWfXJoReBCpgIDAAAAwOdQVn0yNCtZEisCAwAAAEBzKKs+SYqPVV7PHlrPisAAAAAA8DmUVR8Fs1K0oYxpwAAAAADwWZRVHwWzU7Vp1wE1sCIwAAAAAHwKZdVHwawU1TU0advug35HAQAAAICIQln1UTA7tCIwU4EBAAAA4NMoqz4alpUiM2lF8R6/owAAAABARKGs+iglIVanDO2jl5dvV1OT8zsOAAAAAEQMyqrPZk7KVfHugyrYstvvKAAAAAAQMSirPjtvdF8lxQc0e2mx31EAAAAAIGJQVn2WFB+r88f00ysrS1VT3+h3HAAAAACICJTVCDBzYq6qahs0d3WZ31EAAAAAICJQViPA5CG9lZOeyFRgAAAAAPBQViNATIzp0om5mr++XDv31fgdBwAAAAB8R1mNEJdOyFOTk15evt3vKAAAAADgO8pqhBiWlaJx/TM0i6nAAAAAAEBZjSSXTczV2h1VWr19n99RAAAAAMBXlNUIctHYHMUFjLurAAAAALo9ymoE6ZkcrzNHZunl5SVqaGzyOw4AAAAA+IayGmFmTszTrv11enfDLr+jAAAAAIBvKKsR5gsjstQzKY6pwAAAAAC6NcpqhImPjdEXx+Vo7uoy7T1Y73ccAAAAAPAFZTUCzZiYp7qGJr26qtTvKAAAAADgC8pqBBqbl65hWSmazVRgAAAAAN0UZTUCmZlmTMzV4qLd2lJxwO84AAAAANDpKKsR6pLxuTKTZi8t8TsKAAAAAHQ6ymqEysnooZOH9tbsZcVyzvkdBwAAAAA6FWU1gs2cmKdtlQdVsGW331EAAAAAoFNRViPYeaP7Kik+oFlLWGgJAAAAQPdCWY1gyQmxmjamr15ZWaqa+ka/4wAAAABAp6GsRriZE/NUVdugN1aX+R0FAAAAADoNZTXCTRnSWznpiey5CgAAAKBboaxGuJgY0yUTcjV/wy7trKrxOw4AAAAAdArKahcwY2KeGpuc5izf7ncUAAAAAOgUlNUuYFhWisb1z9CspSV+RwEAAACATkFZ7SJmTszVmtJ9Wr19n99RAAAAAKDDUVa7iIvG5iguYCy0BAAAAKBboKx2Eb2S43XGiCz9dfl2NTQ2+R0HAAAAADoUZbULmTkpT7v21+rdwl1+RwEAAACADkVZ7ULOGJGljKQ4zVrCVGAAAAAA0e2oZdXM+pvZO2a2xsw+NrPveOO9zOwNM9vg/dnTGzczu9fMCs1spZlNDLvWV73zN5jZVzvuZ0Wn+NgYfXFcjuauLtPeg/V+xwEAAACADtOSO6sNkr7rnDtO0mRJN5vZKEk/kvSWcy4o6S3v75J0vqSg97pR0gNSqNxKulPSSZJOlHTnoYKLlpsxMU91DU16bVWp31EAAAAAoMMctaw650qdc0u991WS1kjKlTRd0p+80/4k6RLv/XRJT7qQRZIyzKyfpPMkveGcq3TO7Zb0hqRp7fpruoFxeekampms2ey5CgAAACCKteqZVTMbJGmCpA8kZTvnSqVQoZWU5Z2WK2lb2MeKvbHDjaMVzEwzJubpw6JKba2o9jsOAAAAAHSIFpdVM0uRNEvSbc65fUc6tZkxd4Tx5r7rRjMrMLOC8vLylkbsNi6dkCszafYyFloCAAAAEJ1aVFbNLE6hovq0c262N1zmTe+V9+dOb7xYUv+wj+dJ2n6E8c9xzj3knMt3zuVnZma29Ld0GzkZPXTy0N6avbREzjXb9wEAAACgS2vJasAm6VFJa5xzd4cdmiPp0Iq+X5X0ctj4td6qwJMl7fWmCb8u6Vwz6+ktrHSuN4ZjMGNCnrZWVmvJlt1+RwEAAACAdteSO6unSLpG0plmttx7XSDpV5LOMbMNks7x/i5Jr0raJKlQ0sOS/k2SnHOVkn4mabH3+m9vDMdg2pi+6hEX0KylTAUGAAAAEH1ij3aCc+49Nf+8qSSd1cz5TtLNh7nWY5Iea01ANC85IVbnj+mrv68s1Z0Xj1ZiXMDvSAAAAADQblq1GjAiy8xJeaqqadCba8r8jgIAAAAA7Yqy2oVNHtJb/dITNWsJU4EBAAAARBfKahcWiDFdOiFX8zfsUtGuA37HAQAAAIB2Q1nt4r528iDFB2J09xvr/Y4CAAAAAO2GstrFZaUl6vpTB2nOiu36qGSv33EAAAAAoF1QVqPAjacPVXqPOP3m9XV+RwEAAACAdkFZjQLpPeJ08xlDNW99ud7fWOF3HAAAAABoM8pqlLh2yiD1TUvUr19fq9BWtwAAAADQdVFWo0RiXEC3nR3Usq179MZq9l0FAAAA0LVRVqPIZZPyNKRPsn7z+jo1NnF3FQAAAEDXRVmNIrGBGH3vvBHasHO/Zi8t9jsOAAAAABwzymqUOX9MX43NS9f/vblBNfWNfscBAAAAgGNCWY0yZqYfThupkj0H9fQHW/2OAwAAAADHhLIahU4Z1kenDuuj+98pVFVNvd9xAAAAAKDVKKtR6vvnjVDlgTo98u5mv6MAAAAAQKtRVqPUuP4ZuuD4vnrk3U3atb/W7zgAAAAA0CqU1Sj23XNHqKahSfe/U+h3FAAAAABoFcpqFBuamaIv5efp6UVbta2y2u84AAAAANBilNUod+tZQZlJ97y53u8oAAAAANBilNUo1y+9h7528iC9tKxE63ZU+R0HAAAAAFqEstoNfOsLQ5WSEKvfvL7O7ygAAAAA0CKU1W4gIyleN00dqjfXlGnJlkq/4wAAAADAUVFWu4nrThmkzNQE/e9r6+Sc8zsOAAAAABwRZbWbSIqP1a1nBfVhUaX+ub7c7zgAAAAAcESU1W7kihP6a2DvJP36H+vU1MTdVQAAAACRi7LajcQFYnTHOcO1pnSf/rZyu99xAAAAAOCwKKvdzMVjczSqX5p+O3e96hqa/I4DAAAAAM2irHYzMTGmH0wboa2V1Xp+8Va/4wAAAABAsyir3dDU4Zk6aXAv/e6tQlXXNfgdBwAAAAA+h7LaDZmZfjBtpHbtr9XjC4r8jgMAAAAAn0NZ7aYmDeypc0Zl68F/btTuA3V+xwEAAACAT6GsdmPfP2+E9tc16IF5G/2OAgAAAACfQlntxoZnp2rGhDw9sbBIpXsP+h0HAAAAAD5BWe3mbj8nKDnp56+skXPO7zgAAAAAIImy2u3l9UzSd84O6pWVpfrLkmK/4wAAAACAJMoqJN00daimDOmtO1/+WBvL9/sdBwAAAAAoq5ACMab/u2K8EuNi9O1nlqm2odHvSAAAAAC6OcoqJEnZaYm66/JxWl26T796ba3fcQAAAAB0c5RVfOKs47L1tZMH6fEFRXprTZnfcQAAAAB0Y5RVfMqPLxipUf3S9P0XV6psX43fcQAAAAB0U5RVfEpCbED3XjlBB+saddtzy9XYxHY2AAAAADofZRWfMywrRf/1xdF6f1OFHpy30e84AAAAALohyiqadXl+ni4a2093v7FeS7bs9jsOAAAAgG6GsopmmZl+OeN49UtP1K3PLtPeg/V+RwIAAADQjVBWcVhpiXG698oJ2rGvRv/x0io5x/OrAAAAADoHZRVHNHFAT91xznD9fWWpXijY5nccAAAAAN0EZRVHddPUoTp5aG/9dM5qFe6s8jsOAAAAgG7gqGXVzB4zs51m9lHY2PNmttx7FZnZcm98kJkdDDv2YNhnJpnZKjMrNLN7zcw65iehvQViTPd8ebx6xAf07WeXq6a+0e9IAAAAAKJcS+6sPiFpWviAc+7LzrnxzrnxkmZJmh12eOOhY865m8LGH5B0o6Sg9/rUNRHZstMSddflY7WmdJ9+9dpav+MAAAAAiHJHLavOufmSKps75t0d/ZKkZ490DTPrJynNOfe+C63S86SkS1ofF346c2S2rjtlkJ5YWKQ3V5f5HQcAAABAFGvrM6unSSpzzm0IGxtsZsvMbJ6ZneaN5UoqDjun2BtrlpndaGYFZlZQXl7exohoTz86f6RG9UvT919coR17a/yOAwAAACBKtbWsXqlP31UtlTTAOTdB0h2SnjGzNEnNPZ962H1QnHMPOefynXP5mZmZbYyI9pQQG9Dvr5qgmvom3f78cjU2sZ0NAAAAgPZ3zGXVzGIlzZD0/KEx51ytc67Ce79E0kZJwxW6k5oX9vE8SduP9bvhr6GZKfqv6aP1/qYKPfDPQr/jAAAAAIhCbbmzeraktc65T6b3mlmmmQW890MUWkhpk3OuVFKVmU32nnO9VtLLbfhu+OzySXm6eFyO7nlzg5ZsafaRZgAAAAA4Zi3ZuuZZSe9LGmFmxWb2de/QFfr8wkqnS1ppZiskvSjpJufcoSbzLUmPSCpU6I7ra+2QHz4xM/3i0jHKyUjUrc8u196D9X5HAgAAABBFLLQ4b+TKz893BQUFfsfAYSzbuluXP/i+zhvdV/ddNUFsnwsAAACgpcxsiXMuv7ljbV1gCd3chAE9dce5w/XKqlI9MG+j33EAAAAARIlYvwOg67vp9KFat6NKv/7HOuVm9ND08YfdlQgAAAAAWoSyijaLiTH9+rKx2rG3Rt/7ywplpibo5KF9/I4FAAAAoAtjGjDaRUJsQA9dk69BvZP1zT8v0fqyKr8jAQAAAOjCKKtoN+lJcXr8uhOUGBfQdY8v1s59NX5HAgAAANBFUVbRrvJ6Junxr52g3dV1uu6JxTpQ2+B3JAAAAABdEGUV7W5Mbrruv3qi1u6o0s3PLFVDY5PfkQAAAAB0MZRVdIgzRmTpZ9PH6J/ryvWff/1Ikb6fLwAAAIDIwmrA6DBXnTRAJXuqdf87G9W/V5JuPmOY35EAAAAAdBGUVXSo7507Qtv31Og3r69TTkaiLp2Q53ckAAAAAF0AZRUdysz0vzNDe7D+4MWVyk5LZA9WAAAAAEfFM6vocPGxMXrwmkmf7MG6bgd7sAIAAAA4MsoqOkV6jzg9cf2J6hEX0HWPf6gy9mAFAAAAcASUVXSa3IweeuxrJ2jvwXpd9/hi7WcPVgAAAACHQVlFpzq0B+u6sir929NLVc8erAAAAACaQVlFp/vCiCz94pIxmr++XD9hD1YAAAAAzWA1YPjiihMHqGTPQf3+7ULlZvTQt88K+h0JAAAAQAShrMI3d5wzXCW7D+q3b6xXTkYPzZzEHqwAAAAAQiir8I2Z6Vczx2rHvhr9cNZK9U1P1CnD2IMVAAAAAM+swmeH9mAdmpmiG/5UoAWFu/yOBAAAACACUKl3VIAAACAASURBVFbhu7TEOD11w0ka0CtJ1z+xWP9ct9PvSAAAAAB8RllFRMhMTdCzN07W0MwU3fjkEr25uszvSAAAAAB8RFlFxOiVHK9nvzFZx/VL1U1PLdFrq0r9jgQAAADAJ5RVRJT0pNCU4HH9M3TLs8v08vISvyMBAAAA8AFlFREnNTFOT15/ovIH9tRtzy/XXwq2+R0JAAAAQCejrCIiJSfE6onrTtSpw/ro+y+u1DMfbPU7EgAAAIBORFlFxOoRH9DD1+brjBGZ+veXVumJBZv9jgQAAACgk1BWEdES4wJ68JpJOndUtn76t9V6aP5GvyMBAAAA6ASUVUS8hNiA7r96oi4c20+/fHWt7nt7g9+RAAAAAHSwWL8DAC0RF4jR7748XvGBGN01d73qGpp0+znDZWZ+RwMAAADQASir6DJiAzG66/JxiguY7n27ULWNTfrRtJEUVgAAACAKUVbRpQRiTL+aMVbxsTH647xNqmto0v+7aBSFFQAAAIgylFV0OTExpp9NH6O4QIweX1CkuoYm/Wz6GMXEUFgBAACAaEFZRZdkZvp/F4365A5rfWOT/mfGWAUorAAAAEBUoKyiyzIz/WjaSCXEBnTvWxtU19Ckuy4fp9gAi1wDAAAAXR1lFV2amemOc4YrPmC6a+567a9t0O+vnKge8QG/owEAAABoA25BISrccmZQP5s+Wm+t3amrH1mk3Qfq/I4EAAAAoA0oq4ga10wZpD9cNVEflezTZQ8uVPHuar8jAQAAADhGlFVElfOP76cnv36idlbVauYDC7V2xz6/IwEAAAA4BpRVRJ3JQ3rrLzdNkSRd/uD7WrSpwudEAAAAAFqLsoqoNLJvmmZ962RlpSbo2sc+1GurSv2OBAAAAKAVKKuIWnk9k/TiTSdrTE6a/u2Zpfrz+0V+RwIAAADQQpRVRLWeyfF6+obJOmtkln7y8se66/V1cs75HQsAAADAUVBWEfV6xAf04Fcm6cv5/XXfO4X64ayVamhs8jsWAAAAgCOI9TsA0BliAzH61czjlZ2WoHvfLlTF/jrdd9VE9YgP+B0NAAAAQDO4s4puw8x0x7kj9LNLxujtdTt11SOLVHmgzu9YAAAAAJpx1LJqZo+Z2U4z+yhs7KdmVmJmy73XBWHHfmxmhWa2zszOCxuf5o0VmtmP2v+nAC1zzeSBeuDqifp4+z5d9uBCFe+u9jsSAAAAgM9oyZ3VJyRNa2b8HufceO/1qiSZ2ShJV0ga7X3mD2YWMLOApPslnS9plKQrvXMBX0wb009Pff0k7aqq1Yw/LNSa0n1+RwIAAAAQ5qhl1Tk3X1JlC683XdJzzrla59xmSYWSTvRehc65Tc65OknPeecCvjlxcC/95aaTFWOmLz34vt7fWOF3JAAAAACetjyzeouZrfSmCff0xnIlbQs7p9gbO9w44KsRfVM1+99OVnZ6or762Id6ZWWp35EAAAAA6NjL6gOShkoaL6lU0m+9cWvmXHeE8WaZ2Y1mVmBmBeXl5ccYEWiZnIweevGmKRqbl66bn1mq+98pZC9WAAAAwGfHVFadc2XOuUbnXJOkhxWa5iuF7pj2Dzs1T9L2I4wf7voPOefynXP5mZmZxxIRaJWMpHg9dcNJmj4+R795fZ3ueGGFauob/Y4FAAAAdFvHVFbNrF/YXy+VdGil4DmSrjCzBDMbLCko6UNJiyUFzWywmcUrtAjTnGOPDbS/xLiA/u/L4/W9c4frpWUluurhRSqvqvU7FgAAANAttWTrmmclvS9phJkVm9nXJf3azFaZ2UpJZ0i6XZKccx9LekHSakn/kHSzdwe2QdItkl6XtEbSC965QEQxM91yZlB/uHqiVpfu0yX3L9DaHawUDAAAAHQ2i/Rn8/Lz811BQYHfMdANrSzeo288WaD9NQ2698oJOuu4bL8jAQAAAFHFzJY45/KbO9aW1YCBqDY2L0Mv33yqhmSm6IYnC/Tw/E0svAQAAAB0EsoqcAR90xP1wjen6PwxffWLV9foR7NWqa6hye9YAAAAQNSjrAJH0SM+oPuunKhbzxym5wu26ZpHP9DuA3V+xwIAAACiGmUVaIGYGNMd547Q764Yr2Xb9uiSPyxQ4c4qv2MBAAAAUYuyCrTC9PG5eu7GyTpQ26hL71+oeevL/Y4EAAAARCXKKtBKEwf01Mu3nKK8Xkm67vEP9aeFRX5HAgAAAKIOZRU4BrkZPfTiTVN05shs3TnnY/3krx+pvpGFlwAAAID2QlkFjlFyQqz+eM0kfXPqEP150RZd9/hi7a2u9zsWAAAAEBUoq0AbBGJMPz7/OP3msrH6YHOFLv3DAq0vY+ElAAAAoK0oq0A7uDy/v575xmTtPVivC+99V79/awPTggEAAIA2oKwC7eSEQb009/bTdd7ovvrtG+s1/b4F+qhkr9+xAAAAgC6Jsgq0o94pCbrvqon64zWTVL6/VtPvX6C7Xl+n2oZGv6MBAAAAXQplFegA543uqzdvn6pLJ+TqvncKdeG972np1t1+xwIAAAC6DMoq0EHSk+J01+Xj9MR1J6i6tkEzH1ion/99tQ7WcZcVAAAAOBrKKtDBvjAiS6/ffrquPmmAHnlvs6b9br4WbarwOxYAAAAQ0SirQCdITYzTzy85Xs9+Y7Kck654aJF+8tePtL+2we9oAAAAQESirAKdaMrQ3vrHbafp66cO1lMfbNF598zX/PXlfscCAAAAIg5lFehkSfGx+slFo/TiTScrMS5G1z72oX7w4grtPVjvdzQAAAAgYlBWAZ9MGthTr9x6mm4+Y6hmLS3ROXfP0xury/yOBQAAAEQEyirgo8S4gL5/3ki9fPMp6pUcr288WaCbn16qjeX7/Y4GAAAA+IqyCkSAMbnpmnPLqbrjnOF6a22Zzrl7nr7z3DIV7qzyOxoAAADgC3PO+Z3hiPLz811BQYHfMYBOs2t/rR5+d5P+/P4WHaxv1IXH99O3zwxqRN9Uv6MBAAAA7crMljjn8ps9RlkFIlPlgTo98u4m/WlhkQ7UNer8MX1161lBHdcvze9oAAAAQLugrAJd2J7qOj323mY9vqBIVbUNOndUtm49K6gxuel+RwMAAADahLIKRIG91fV6fOFmPfbeZu2radDZx2Xp1rOCGpuX4Xc0AAAA4JhQVoEosq+mXn9aUKRH3tusvQfrdcaITN16VlATBvT0OxoAAADQKpRVIApV1dTrz4u26OH5m7S7ul6nBfvotrODmjSwl9/RAAAAgBahrAJR7EBtg55atEUPzd+kigN1OmVYb/1w2kimBwMAACDiUVaBbqC6rkHPfLBVD87bpMoDtbrulMH67rnDlRQf63c0AAAAoFlHKqsxnR0GQMdIio/VDacN0dvfm6orTxygR9/brHPvma9568v9jgYAAAC0GmUViDJpiXH6xaXH64VvTlF8bIy++tiHuuP55ao8UOd3NAAAAKDFKKtAlDpxcC+9eutp+vaZwzRnxXadffc8vby8RJE+9R8AAACQKKtAVEuMC+i7547Q3289VQN6Jek7zy3XdU8sVvHuar+jAQAAAEdEWQW6gZF90zTrWyfrzotH6cPNlTr3nvl6fMFmNTZxlxUAAACRibIKdBOBGNN1pwzW3NtP14mDe+m//rZaMx9YqHU7qvyOBgAAAHwOZRXoZvJ6Junxr52g310xXlsrq3Xhve/q7rnrVFPf6Hc0AAAA4BOUVaAbMjNNH5+rN++Yqi+Oy9G9bxfqgnvf1YebK/2OBgAAAEiirALdWq/keN395fF68voTVdfQpC/98X39x0urtK+m3u9oAAAA6OYoqwB0+vBMzb39dN1w6mA9++FWnXnXPN31+joV7TrgdzQAAAB0Uxbpey7m5+e7goICv2MA3caKbXt0z5vrNX99uZqcdNLgXro8v78uOL6vkuJj/Y4HAACAKGJmS5xz+c0eo6wCaM6OvTWatbRYfynYpqKKaqUkxOqisf10eX5/TRyQITPzOyIAAAC6OMoqgGPmnNPiot16oWCbXl1Vquq6Rg3NTNbl+f01Y2KuslIT/Y4IAACALoqyCqBd7K9t0KsrS/VCwTYVbNmtQIzpjBGZujy/v84cmaW4AI/BAwAAoOUoqwDa3cby/fpLQbFmLy3Wzqpa9UmJ1yXjc/WlE/preHaq3/EAAADQBVBWAXSYhsYmzd9QrhcWF+vNNWVqaHIa1z9DX50yUNPH5yoQw7OtAAAAaB5lFUCnqNhfq5eWleiFgm1aX7ZfwawUfffcETpvdDYLMgEAAOBzjlRWj/qAmZk9ZmY7zeyjsLHfmNlaM1tpZi+ZWYY3PsjMDprZcu/1YNhnJpnZKjMrNLN7jf/nCkSd3ikJuuG0IfrHd07X/VdNVKNzuumpJbrk/gV6b8MuRfp/HAMAAEDkaMlqKE9ImvaZsTckjXHOjZW0XtKPw45tdM6N9143hY0/IOlGSUHv9dlrAogSMTGmC8f209zbTtevLxurXfvr9JVHP9BVD3+gpVt3+x0PAAAAXcBRy6pzbr6kys+MzXXONXh/XSQp70jXMLN+ktKcc++70K2VJyVdcmyRAXQVsYEYfSm/v97+3lTdefEorS+r0ow/LNQNfyrQ2h37/I4HAACACNYe+0xcL+m1sL8PNrNlZjbPzE7zxnIlFYedU+yNAegGEmIDuu6UwZr/gzP0vXOH64PNFTr/d+/qO88t05aKA37HAwAAQASKbcuHzew/JDVIetobKpU0wDlXYWaTJP3VzEZLau751MM+vGZmNyo0ZVgDBgxoS0QAESQ5IVa3nBnUVyYP1IPzNumJhZv1yspSfemE/rr1zKD6pif6HREAAAAR4pjvrJrZVyVdJOlqb2qvnHO1zrkK7/0SSRslDVfoTmr4VOE8SdsPd23n3EPOuXznXH5mZuaxRgQQoTKS4vWj80dq/vfP0JUnDtALi7dp6m/e0S9fXaPKA3V+xwMAAEAEOKayambTJP1Q0hedc9Vh45lmFvDeD1FoIaVNzrlSSVVmNtlbBfhaSS+3OT2ALi0rLVE/u2SM3v7uF3Th2H56+N1NOv3X7+h3b27Q/tqGo18AAAAAUeuo+6ya2bOSviCpj6QySXcqtPpvgqQK77RFzrmbzGympP9WaGpwo6Q7nXN/866Tr9DKwj0Uesb1264F+1iwzyrQfawvq9Jv567T6x+XqWdSnK6dMkjXTBmoPikJfkcDAABABzjSPqtHLat+o6wC3c+KbXv0u7c26O21OxUfG6MZE3J1/amDNTw71e9oAAAAaEeUVQBdUuHO/Xp8wWa9uKRYtQ1Nmjo8U18/dbBOC/ZR6IkCAAAAdGWUVQBdWuWBOj29aIv+9P4W7dpfqxHZqfr6aYM1fXyOEmIDfscDAADAMaKsAogKtQ2NmrN8ux59b7PW7qhSn5QEXTtloK4+aYB681wrAABAl0NZBRBVnHNaUFihR97bpH+uK1dCbIxmTMzT108dpGFZPNcKAADQVRyprMZ2dhgAaCsz06nBPjo12Ecbyqr02ILNmrW0WM9+uFVnjMjUDacN0clDe/NcKwAAQBfGnVUAUaFif62eWrRVf15UpF376zSyb6ouz++vcXnpGpWTpqR4/tscAABApGEaMIBuo6a+UXNWbNej727WurIqSVKMSUMzU3R8brrG5Kbr+Lx0jeqXpuQECiwAAICfKKsAuqWyfTVaVbxXq0r26qOS0J87q2olSfbZApsbugObQoEFAADoNDyzCqBbyk5LVPaoRJ09KvuTsZ37arSq5F8FduHGXXppWYmkUIEd3CdZx3vl9fjcdI3rn6HEOLbHAQAA6GyUVQDdSlZaos5KS9RZx4UV2Kqa0J3X4n1aVbJXH2yq1MvLt0uSEuNiNGVIb00dnqnTh2dqcJ9kFm4CAADoBJRVAN1eVmqizhyZqDNH/qvAllfVavm2PXpvQ7nmb9ild/62WpLUv1cPTR2eqanDszRlaG+mDQMAAHQQnlkFgBbYUnFA89eXa976ci3cWKHqukbFBUyTBvbU1OFZmjo8U8f1S+WuKwAAQCuwwBIAtKO6hiYVbKnUvPXlmreuXGt3hFYdzkxN0OnBTE0dkanThvVRz+R4n5MCAABENsoqAHSgsn01n9x1fa9wl/ZU18tMGpuXodODfXTCoF6aMCBDqYlxfkcFAACIKJRVAOgkjU1OK4v3hO66ri/Xim171ORCe70e1y9NJwzq5b16Kist0e+4AAAAvqKsAoBP9tc2aNnW3VpctFsFRZVatnWPDtY3SpIG9EpS/qCeOnFQL+UP6qWhmaw0DAAAuhf2WQUAn6QkxOq0YKZOC2ZKkuobm7R6+z4tLqrU4qJKzVtXrtlLQ/u89kqO16SBh8prT43OSVd8bIyf8QEAAHzDnVUA8JFzTpt3HVBB0W59WFSpgqJKFVVUSwrt8Tq+f4YuPL6fLp2YxzY5AAAg6jANGAC6kJ1VNVpSFJo6vHDjLq3dUaXk+IBmTMzTVyYP1Ii+qX5HBAAAaBeUVQDowlZs26M/L9qiOSu2q66hSScO7qVrJg/UeaP7Mk0YAAB0aZRVAIgCuw/U6S9LtumpRVu1tbJafVISdOWJ/XXliQOUk9HD73gAAACtRlkFgCjS1OQ0f0O5nlq0RW+t3SmTdPZx2bpmykCdMrSPYmJYURgAAHQNrAYMAFEkJsb0hRFZ+sKILG2rrNazH27V84u3ae7qMg3pk6yrJw/UZRPzlJ4U53dUAACAY8adVQCIArUNjfrHRzv05PtbtGTLbiXGxWj6uFxdM2WgxuSm+x0PAACgWUwDBoBu5OPte/XUoq3667ISHaxv1Lj+GbpsUp4uHttPGUnxfscDAAD4BGUVALqhfTX1mr2kWM9+uE3ryqoUH4jRWcdlaebEPE0dkam4ACsJAwAAf1FWAaAbc87p4+37NHtpiV5eXqKKA3XqnRyvL47P0cyJeRqdkyYzFmUCAACdj7IKAJAk1Tc2af76cs1aWqw3V+9UXWOTRmSnauakXF0yPldZaYl+RwQAAN0IZRUA8Dl7q+v1t5XbNWtpsZZt3aMYk04LZmrGxFydN7qvEuMCfkcEAABRjrIKADiiTeX7NXtpiV5aVqKSPQeVmhCrC47vp5mT8nTCoJ5MEwYAAB2CsgoAaJGmJqdFmys0e2mJXl1Vquq6RvXv1UOnDsvUkD7JGtwnWYMzk9W/Z5LiY1mgCQAAtA1lFQDQatV1DfrHRzv00rISfVSyV7ur6z85Fogx9e/ZI1Re+6RocJ+k0J+ZyeqXlqiYGO7EAgCAoztSWY3t7DAAgK4hKT5WMybmacbEPEnSnuo6bd514JPXpl0HtLn8gBZtqtTB+sZPPpcQG+OV2H+9gtmpOj43XQFKLAAAaCHKKgCgRTKS4jVhQLwmDOj5qXHnnMr21WrTrv0q2lWtzbv2a/OuA1pXVqU3VpepoSk0gyc7LUEXHp+jL47P0bi8dJ6DBQAAR0RZBQC0iZmpb3qi+qYn6uShnz7W0Nik4t0HtbJkr/6+YrueWrRFjy3YrP69eujisTm6eFyORvZNpbgCAIDP4ZlVAECn2VdTr7kfl2nOiu1aULhLjU1OwawUXTwuVFwH90n2OyIAAOhELLAEAIg4Fftr9dpHOzRnxXYtLqqUc9Lxuem6eFw/XTQ2RzkZPfyOCAAAOhhlFQAQ0Ur3HtQrK0v1txXbtaJ4ryTphEE9dfG4HJ0/pp8yUxN8TggAADoCZRUA0GVsqTigv68s1Zzl27WurEoxJp0yrI/yB/ZSckJAyQmxoVd8QEnxsf8ai49VUkJAyfGxrDoMAEAXQVkFAHRJ63ZU6W8rtuvvK7erqKK6xZ9LjIv5VHlNTohVSkKsRvRN1Zjc9P/f3r0Gx3Xe9x3//c9eAexicVncCALg/WaREiWZis2YlhXHUWW1yYu4004647aZUV60HXeaJnXczmSa1h03L9pm2ryIJ3XjZFI7niZpHMlxq1rSSCPfRJEWSfMigjeQBIj7bRfAXp++OAcLgCIpUSS4C+73M3PmPOfZg3Mecp7h8ofnOc/R/t6UBtoaeR8sAABVRlgFAGx4xVJZ2XxJC/misjl/n8kVtZArKbuqLls5LmohX1I2V1Q2X9TMQkHnxzLKF8uSpGQ8rEc2pbR/c0qP9KZ0oDelgfZGViYGAOABulNY5dU1AIANIRzylGrwlGqIfOhrFEplvTs6r1PXZ3Xi2qxOXZ/VH715WfnS2gB7IAiw+wmwAABUDSOrAIC6li+uBNiTwXZ2ZH5NgN0fBNeD/S362Pb0PQVmAACwgmnAAADcheUAuxxeT60KsCHP9Fhfi47s7NCRXWkd2NzCgk4AAHxIhFUAAO5RvljWT67O6PV3x/X6+XGdvD4r56SWxogO70jrkzs79IldafWkeD8sAAAfFGEVAID7bCqb1xvnx/X6uxN64/y4xuZzkqSdnQkd2dWhI7s69NTWNsUjoSq3FACA2kVYBQBgHTnndG503h91fXdCP740pXyprFjY06GtbfpkEF53diZYrAkAgFXuOaya2dckPS9pzDn3SFDXJunPJG2RdFnS33XOTZv/Lfx7kp6TtCDpHzrnjgU/83lJ/ya47L93zn39/e5NWAUAbDSL+ZJ+eGkyCK/jujCelSR1N8e1tyep/rZG9bc3aaCtUf3tjepva2QEFgBQl+5HWD0iKSPpj1eF1d+VNOWc+4qZfVFSq3PuX5nZc5L+mfyw+pSk33POPRWE26OSnpTkJL0t6Qnn3PSd7k1YBQBsdNdnFvX6u+N6c3BCF8ezGppaUCZXXHNOZzKmgfZG9bU1aqCtaaXc3qj2pigjsgCAh9J9mQZsZlskvbgqrJ6T9LRzbsTMeiS95pzbbWZ/EJS/sfq85c0592tB/ZrzboewCgB42DjnNL1Q0JVJP7gOTS5oaGpBV4LyjbmlNec3RUPqa/NHYLekm7SjI6HtnQnt6EzwGh0AwIZ2p7AavofrdjnnRiQpCKydQX2vpKurzrsW1N2uHgCAumJmamuKqq0pqoP9re/5fKlQ0rXpIMAGQXZockGXJrJ67d1x5YvlyrmdyZh2dCa0MwivOzqT2tGZUDrBaCwAYGO7l7B6O7f6ZnR3qH/vBcxekPSCJPX399+/lgEAsAHEI6EgdCbf81mp7HR1akGDYxkNjmd0ftTf//mx62umFqcaIqsC7Mq2KdUgj/fCAgA2gHsJq6Nm1rNqGvBYUH9NUt+q8zZLGg7qn76p/rVbXdg591VJX5X8acD30EYAAB4qIc+0Jd2kLekmfVpdlXrnnG7MLfkhdiyj88H+5dOj+uZbKxObGiIhtTRGFA6ZIiFPEc9TOGQKhzxFPFupD3kKe345HDKFPU+RkP95Uyys/b0pPd7fqp5UnBFcAMC6uJew+m1Jn5f0lWD/V6vq/6mZfVP+AkuzQaD9P5L+g5ktz3f6jKTfuof7AwCAgJmpJ9WgnlSDPrGzY81nU9l8JcQOjmU0t1RQsVRWoexULJVVLLlKuVAqa6lQVLEc1JfKKpadCsWV87O5kvIlfypyV3NMB/ta9fhAiw72t2p/b4qVjQEA98UHCqtm9g35o6JpM7sm6bflh9RvmdmvShqS9Lng9O/IXwl4UP6ra/6RJDnnpszs30l6Kzjvd5xzU/fpzwEAAG6jrSmqQ1vbdGhr2325XqFU1pmROR0fmtHxoWkdG5rRd396Q5IU9kz7NjXrYF+LHh9o1cG+VvW1NTD6CgC4ax94NeBqYTVgAABq30QmVwmvx4dm9M61GS3kS5KkdCKqx/padbC/RY/3t+rA5pSaYuuxbAYAYKNZr9WAAQAAJEnpREw/v69LP7/Pf462WCrr3dGMjl+d1rErMzp+dVr/78yoJMkzqbs5rkQ8rKZYWIlYWMm4v0/EIkrEw0rGwpXPl8uJm84Nh7xq/pEBAOuMsAoAAO67cMjTvk3N2repWb/y1IAkaTqb10+uzej40IxGZhaVyRUr243ZJb+8VFQmX9T7TfwKeabH+1v0qT2d+tTuTu3pTjLVGAAeMkwDBgAANaVcdloolPzgmitofskPtNlcsVIencvpjfPj+unwnCSpJxXX07s79cyeTh3e0a7GKL+PB4CN4E7TgAmrAABgwxqdW9Jr58b06tlxvXF+XNl8SdGQp6e2temZYNR1S7qp2s0EANwGYRUAADz08sWyjl6e0itnx/TquTFdGM9Kkralmyqjrh/d2qpYmFfrAECtIKwCAIC6MzS5oFfPjemVs2P6wcVJ5YtlNUVDOrwjrWf2dOrJLa3qao4rEQs/sOddc8WSbswuaXhmScMziyo5pycHWrU13cQztwDqEmEVAADUtcV8Sd+/MKFXgynD12cWK581RELqao6pMxlXR3NMXcm4OptjlbrOZEydzXE1x+8caktlp7H5lSA6Mruo4ZmlNfuJTP6WP5tOxPTU1jZ9dEurDm1t157upDyP8Arg4UdYBQAACDjn9O5oRmdG5jQ2v6SxuZxG53Mam1vSWLDPBu+IXS0W9tTVvBxeY2pvimlmsaCRmUUNzyxqdD6nUnnt/6sSsbB6UnFtamnQppa4elIN6knF1dvSoJ6WBpXKZb11eVo/vjSlH1+aqoTo5nhYH93SpkNb/e2R3pQiNfiqHueczo9l1NYUVToRq3ZzAGxAhFUAAIC7kMkVK+F1dG5J4/O5StkPt0uazOTV0hjxw2iqQZtaGtTTsrbcHI/c1X2vTS/orct+cP3RpSldDJ67bYiE9MRAayW8PtbXonikOs/eOud0bnReL74zopdOjujSRFYNkZBeOLJNLxzZpqYYKzED+OAIqwAAABvQ+HxuTXg9e2NOzknRkKcDm1M6tLVNH93SpoP9LWppjK5rWwbH5vXiiRG9eGJEg2MZeSZ9bHu7nn2kRz+8OKmXToyoMxnTr39ml375iT6FmMYM4AMgrAIAADwEZhcKOnplJbyeuj6rYjD1eGdnQk8MWy1hUwAADJ1JREFUtOrxgVY9MdCqbfdh0abLE1m9eGJYL54Y0dkb8zKTDm1p0/OPbtKzH+lWR3Jl6u/bV6b15ZdO69jQjHZ3JfWlz+7VJ3d13NP9ATz8CKsAAAAPoYV8Ue9cndWxoWkdvTylY0Mzml0sSJJaGyOV8PrkQJsObE59oKnDV6cW9NLJEb14Ylinrs9Jkp4YaNXzB3r03P4edTXHb/uzzjn9zakb+srfnNXQ1II+sTOtf/3ZvdrT3Xx//sAAHjqEVQAAgDpQLjtdnMjo6OVpvX1lWm8PTVeeew17po/0pvREf6ue3OKPvi4Hz+GZRX3n5Ij++sSI3rk6I0l6tK9FfzsIqJtaGu6qHbliSX/ygyv6r68Man6poM890ad/8Zlddwy6AOoTYRUAAKBOTWXzOj40raNX/AD7ztUZ5YplSVJvS4PSiajeuTYrSfrIpmY9f2CTnj/Qo762xnu+98xCXv/tlUF9/QeXFfY8vXBkm37tk9vUGGURJgA+wioAAAAkSfliWWdG5nT0yrSOXZnWyOyiPrW7U88/uklb003rcs8rk1n97nfP6aWT928RptmFgi5MZHRxPKuL4xl5Ztrb06y9PUkNtDexwBOwQRBWAQAAUHVvX5nSl186o2NDM9rTndSXnturI3dYhClfLGtoakEXxzO6OOGH0ovjWV2ayGoym6+cF/ZMTqq857YhEtLu7qT2bWrW3p5m7etJand3sxK8VgeoOYRVAAAA1ATnnL5z8oa+8t0zujq1qCO7OvSFn9upQqlcGSW9NJHVxYmshqYWKgFUktKJqLalE9rW0aRtHU3aGpT72xpVKjsNjmV0emROp4fndGbE3+aWipWfH2hv1L6e5mAE1h+F7W1puOdVkwF8eIRVAAAA1JTVizAtr2AsSbGwp61pP4xuSydWyh0JpRoid3UP55yGZ5d0ZnhOp0dWAuzlyYXKOc3xcCW89qTiSidi6kjGlE7ElE5G1d4UY0oxsI4IqwAAAKhJMwt5vXx6VJ3NcW1LN6m3pUHeOofDbK6oszfm1wTYczfmtZAvvedcz6S2pqgfXitB9uZjgi3wYRFWAQAAgDtwzimTK2oik9dEJqfx+ZwmMjlNzOc0nsmvHAefLa+ovFoiFtZTW9v08R1pHd7Rrt1dSaYYA+/jTmGVp8wBAABQ98xMyXhEyXjkfVdFvjnY+oE2p3M35vX9C5P63tkxSf4zth/bntbh7e06vCN9X14HdDuFkr8Y1dxiQbu6kmpiMSk8BOjFAAAAwF14v2B7fWZR3x+c0JuDE3rzwqT++p1hSVJfW4MOb0/r4zvS+vj2dqUTsbu+9/Irey6MZXRhPKsL4xldHM/oyuSCisFiVGbS9o6EDvSm9EhvSvs3p/SRTc283xYbDtOAAQAAgHXinL9K8XJw/eHFSc0HKxTv6U7qcDBl+NDW9sqrdUplp+GZRQ2Or4TSi+P+fiKTq1w7EjINtDdpe0eTtncktL0joWQ8rNMjczp1fVYnrs1qbN4/3wsC7P4gvO7vTWkfARY1gGdWAQAAgBpQLJV1anjOD6+DEzp6ZVr5Yllhz/RIb0pLhZIuTWTXPBPb0hgJwuhKKN3emVBfa4PCIe+O9xubW9LJILieuj6rE9dnNX5zgA3C6+oAWyyVtVAoaSlf0kKwLRZKWgz2C/niqnJpTTlfLCse8dQYDakhGlZjNBRs4aAupMaIf9xQ+cyvj4Y8nvOtM4RVAAAAoAYtFUp6+8q03hyc0FuXp5SMR1ZCaacfTNuaovf1nqNzSzp5bVYnr69sqwNs2POUL713Aak78UxqjIYVj4QUC3taKqwE3LsR8kyNkZC6U/HKK4X2bfLfiduZjN/VtbAxEFYBAAAA3NbqAJsrlv2RzkioMvK5Ug6vqW+MhioB9VYjouWy01KxpGzOH31dKBQrI7H+iG2xUl4slJTN+Z9fm17QmZF5XZ9ZrFwrnYj64TUIsXt7mrWto0mR9xldRm1jNWAAAAAAt9XVHFfXvrg+va/rvl7X8yyY/vvhYsfMQl5nRuYr78M9PTKn//Hm5crIbzTsaVdXQnu7l0dg/S3VELntNZ1zKpScCqWyiiWnfKmsYnlVOfgsGvbUnYorGQszNblKGFkFAAAAsGEUSmVdHM9WwuuZkTmdHp7TZDZfOaerOaaw5/mBtOxUKJZVKJdVKDmVyneXfxqj/rTk7ua4ulNx9VTKDZW69qaoPI9A+2EwsgoAAADgoRAJedrdndTu7qR+6WCvJH+0dHw+F4TXeV0Yz8g5KRo2hT1P4ZApGvL3kZCnSMhT2Fsu+/vw6rJnWiqWNTq7pJHZJY3OLWlkdlE/vDCp0fncewJvJGTqTPrBtTsVV08QYpNx/zneNYtLRUNqjIQVj3qVadUhgu4tEVYBAAAAbGhmps7muDqb43p6d+e63qtUdprM5DQyu6Qbc0u6cVOgPT08p++dGdVS4YMvUhULe5UVk/2VlP2VkmNhT6WyU7Hsjwgvb/5xec1xefV5zqlU8o//8c9u0W/8wp51/BtZP4RVAAAAAPiAQt5KMH70Nuc45zS3WFQmX9Rivrjy+p81r/sprnkl0MorglYWncrkigp7ppBnikQ8hTyvchwyUyhk/rH5deGQybOgzvMU8qTH+1sf6N/P/URYBQAAAID7yMyUaowo1Xj7hZ7w/ljnGQAAAABQcwirAAAAAICaQ1gFAAAAANQcwioAAAAAoOYQVgEAAAAANYewCgAAAACoOYRVAAAAAEDNIawCAAAAAGoOYRUAAAAAUHMIqwAAAACAmkNYBQAAAADUHMIqAAAAAKDmEFYBAAAAADWHsAoAAAAAqDmEVQAAAABAzSGsAgAAAABqDmEVAAAAAFBzCKsAAAAAgJpjzrlqt+GOzGxc0pV1vEVa0sQ6Xh+4W/RJ1CL6JWoR/RK1hj6JWlTr/XLAOddxqw9qPqyuNzM76px7strtAJbRJ1GL6JeoRfRL1Br6JGrRRu6XTAMGAAAAANQcwioAAAAAoOYQVqWvVrsBwE3ok6hF9EvUIvolag19ErVow/bLun9mFQAAAABQexhZBQAAAADUnLoNq2b2rJmdM7NBM/titduD+mRmXzOzMTM7taquzcxeNrPzwb61mm1EfTGzPjN71czOmNlPzewLQT39ElVjZnEz+7GZvRP0y38b1G81sx8F/fLPzCxa7baivphZyMyOm9mLwTF9ElVlZpfN7KSZ/cTMjgZ1G/Y7vC7DqpmFJP2+pL8laZ+kv29m+6rbKtSpP5L07E11X5T0PefcTknfC46BB6Uo6dedc3sl/YykfxL8+0i/RDXlJD3jnHtU0mOSnjWzn5H0HyX956BfTkv61Sq2EfXpC5LOrDqmT6IWfMo599iq19Vs2O/wugyrkg5JGnTOXXTO5SV9U9IvVrlNqEPOudclTd1U/YuSvh6Uvy7plx5oo1DXnHMjzrljQXle/n/CekW/RBU5XyY4jASbk/SMpP8V1NMv8UCZ2WZJn5X0h8GxiT6J2rRhv8PrNaz2Srq66vhaUAfUgi7n3IjkBwdJnVVuD+qUmW2RdFDSj0S/RJUF0y1/ImlM0suSLkiacc4Vg1P4LseD9l8k/aakcnDcLvokqs9J+r9m9raZvRDUbdjv8HC1G1Aldos6lkUGgICZJST9uaR/7pyb8wcMgOpxzpUkPWZmLZL+UtLeW532YFuFemVmz0sac869bWZPL1ff4lT6JB60w865YTPrlPSymZ2tdoPuRb2OrF6T1LfqeLOk4Sq1BbjZqJn1SFKwH6tye1BnzCwiP6j+qXPuL4Jq+iVqgnNuRtJr8p+pbjGz5V+8812OB+mwpL9jZpflP072jPyRVvokqso5Nxzsx+T/Yu+QNvB3eL2G1bck7QxWbItK+nuSvl3lNgHLvi3p80H585L+qoptQZ0Jnrn675LOOOf+06qP6JeoGjPrCEZUZWYNkj4t/3nqVyX9cnAa/RIPjHPut5xzm51zW+T/P/IV59yviD6JKjKzJjNLLpclfUbSKW3g73Bzrj5nJ5jZc/J/AxaS9DXn3Jer3CTUITP7hqSnJaUljUr6bUn/W9K3JPVLGpL0OefczYswAevCzH5W0huSTmrlOawvyX9ulX6JqjCzA/IXBQnJ/0X7t5xzv2Nm2+SParVJOi7pHzjnctVrKepRMA34XzrnnqdPopqC/veXwWFY0v90zn3ZzNq1Qb/D6zasAgAAAABqV71OAwYAAAAA1DDCKgAAAACg5hBWAQAAAAA1h7AKAAAAAKg5hFUAAAAAQM0hrAIAAAAAag5hFQAAAABQcwirAAAAAICa8/8BXPJbsyZU6LkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, train_iterator, optimiser, criterion, epochs, force_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state_dict:\n",
      "encoder.embedding.weight \t torch.Size([4245, 256])\n",
      "encoder.lstm.weight_ih_l0 \t torch.Size([2048, 256])\n",
      "encoder.lstm.weight_hh_l0 \t torch.Size([2048, 512])\n",
      "encoder.lstm.weight_ih_l0_reverse \t torch.Size([2048, 256])\n",
      "encoder.lstm.weight_hh_l0_reverse \t torch.Size([2048, 512])\n",
      "encoder.ff.0.weight \t torch.Size([512, 1024])\n",
      "encoder.ff.0.bias \t torch.Size([512])\n",
      "decoder.embedding.weight \t torch.Size([3813, 256])\n",
      "decoder.attention.W.weight \t torch.Size([512, 1536])\n",
      "decoder.attention.W.bias \t torch.Size([512])\n",
      "decoder.attention.v.weight \t torch.Size([1, 512])\n",
      "decoder.lstm.weight_ih_l0 \t torch.Size([2048, 1280])\n",
      "decoder.lstm.weight_hh_l0 \t torch.Size([2048, 512])\n",
      "decoder.lstm.bias_ih_l0 \t torch.Size([2048])\n",
      "decoder.lstm.bias_hh_l0 \t torch.Size([2048])\n",
      "decoder.ff.0.weight \t torch.Size([256, 1792])\n",
      "decoder.ff.0.bias \t torch.Size([256])\n",
      "decoder.output_layer.weight \t torch.Size([3813, 256])\n",
      "decoder.output_layer.bias \t torch.Size([3813])\n",
      "Optimiser state_dict:\n",
      "state \t {2473065133312: {'step': 29000, 'exp_avg': tensor([[ 4.3731e-05,  2.2455e-04,  9.6892e-05,  ...,  3.8204e-04,\n",
      "         -1.1450e-05,  1.3460e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 2.1100e-04,  1.0050e-04, -1.6877e-04,  ..., -1.0798e-05,\n",
      "         -9.4022e-05,  1.7047e-04],\n",
      "        ...,\n",
      "        [-1.2988e-09, -4.6042e-10,  7.2610e-10,  ...,  1.0709e-12,\n",
      "          1.0882e-10,  1.5979e-12],\n",
      "        [-3.8236e-07, -5.9367e-06, -9.1959e-14,  ...,  3.5087e-06,\n",
      "         -1.5516e-06, -1.5755e-06],\n",
      "        [-9.3356e-06, -1.4425e-05,  7.5088e-22,  ..., -3.3991e-06,\n",
      "         -9.7594e-08,  1.1812e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[4.4482e-07, 4.7036e-07, 3.2615e-07,  ..., 4.2452e-07, 4.8392e-07,\n",
      "         4.8187e-07],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.7402e-07, 2.3782e-07, 2.4230e-07,  ..., 2.2218e-07, 2.7877e-07,\n",
      "         2.9202e-07],\n",
      "        ...,\n",
      "        [5.9146e-10, 6.7381e-10, 7.1299e-10,  ..., 2.9280e-10, 5.7200e-10,\n",
      "         6.5145e-10],\n",
      "        [4.8517e-10, 1.2933e-09, 9.2336e-10,  ..., 9.2425e-10, 7.9722e-10,\n",
      "         9.2023e-10],\n",
      "        [4.4368e-10, 4.4758e-10, 3.2987e-10,  ..., 3.9840e-10, 5.2160e-10,\n",
      "         4.8540e-10]], device='cuda:0')}, 2472885619008: {'step': 29000, 'exp_avg': tensor([[-1.1850e-04, -1.0969e-05,  3.3366e-06,  ..., -1.4058e-04,\n",
      "          5.1845e-05,  5.4966e-05],\n",
      "        [ 3.2084e-05, -5.7505e-05, -5.9229e-05,  ...,  2.1387e-04,\n",
      "          5.2344e-05, -6.5347e-05],\n",
      "        [-3.8241e-05,  2.5214e-05,  6.5184e-05,  ..., -5.8005e-05,\n",
      "          7.6639e-05, -8.8498e-05],\n",
      "        ...,\n",
      "        [ 6.4227e-05,  5.3906e-05, -5.1546e-05,  ..., -1.2625e-04,\n",
      "         -5.0463e-05, -1.6941e-05],\n",
      "        [ 2.8595e-05,  1.3267e-04, -2.0814e-05,  ..., -4.1413e-05,\n",
      "          2.6052e-05, -2.6883e-05],\n",
      "        [ 3.6439e-05, -4.1905e-05, -4.9689e-07,  ..., -1.9170e-04,\n",
      "          5.2233e-05, -4.2023e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[1.0659e-07, 9.5587e-08, 1.3722e-07,  ..., 1.0940e-07, 1.0772e-07,\n",
      "         1.0281e-07],\n",
      "        [1.2354e-07, 1.1043e-07, 1.5494e-07,  ..., 1.2294e-07, 1.4229e-07,\n",
      "         1.0706e-07],\n",
      "        [1.3795e-07, 1.3665e-07, 1.5053e-07,  ..., 1.3551e-07, 1.5660e-07,\n",
      "         1.1959e-07],\n",
      "        ...,\n",
      "        [5.5599e-08, 4.6760e-08, 5.6076e-08,  ..., 5.8281e-08, 5.2527e-08,\n",
      "         4.6008e-08],\n",
      "        [6.1927e-08, 5.8622e-08, 6.6431e-08,  ..., 6.5064e-08, 5.8751e-08,\n",
      "         5.8110e-08],\n",
      "        [1.3363e-07, 9.6353e-08, 1.5633e-07,  ..., 1.3710e-07, 1.0619e-07,\n",
      "         1.0593e-07]], device='cuda:0')}, 2472885603840: {'step': 29000, 'exp_avg': tensor([[ 6.6397e-05, -3.8441e-05, -1.4952e-05,  ...,  8.2438e-06,\n",
      "          2.1921e-05,  3.3464e-06],\n",
      "        [-2.4840e-05, -7.9083e-06, -8.6339e-05,  ...,  1.6366e-05,\n",
      "          6.4942e-06,  1.6863e-05],\n",
      "        [-4.3047e-05, -8.1091e-06,  1.2662e-05,  ..., -9.7880e-06,\n",
      "         -3.1427e-06, -3.8316e-05],\n",
      "        ...,\n",
      "        [-5.5636e-05, -1.9029e-06,  8.0238e-06,  ...,  2.2857e-05,\n",
      "          4.4894e-06,  8.3964e-07],\n",
      "        [ 3.8132e-05, -3.8370e-06,  1.6607e-05,  ..., -5.1290e-06,\n",
      "         -2.4593e-05, -2.7044e-06],\n",
      "        [ 2.7118e-05,  4.1886e-05,  2.0072e-05,  ..., -2.2362e-05,\n",
      "          9.3178e-07,  9.9973e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[1.3150e-08, 1.3326e-08, 1.0522e-08,  ..., 9.0621e-09, 1.4547e-08,\n",
      "         9.3977e-09],\n",
      "        [1.7015e-08, 1.0122e-08, 1.2405e-08,  ..., 8.5553e-09, 1.4585e-08,\n",
      "         1.1975e-08],\n",
      "        [9.8555e-09, 1.2481e-08, 1.6166e-08,  ..., 1.2281e-08, 1.2722e-08,\n",
      "         1.0379e-08],\n",
      "        ...,\n",
      "        [8.3483e-09, 6.1071e-09, 5.5590e-09,  ..., 1.0263e-08, 7.6557e-09,\n",
      "         4.8519e-09],\n",
      "        [1.6638e-08, 8.6611e-09, 6.7606e-09,  ..., 7.7401e-09, 1.8586e-08,\n",
      "         8.2970e-09],\n",
      "        [1.5108e-08, 1.4905e-08, 1.4444e-08,  ..., 1.4170e-08, 1.6088e-08,\n",
      "         1.1175e-06]], device='cuda:0')}, 2472662862336: {'step': 29000, 'exp_avg': tensor([[ 9.3478e-05,  1.2063e-05, -9.7918e-05,  ...,  1.3913e-04,\n",
      "         -1.1144e-04, -1.1068e-04],\n",
      "        [ 1.3566e-04, -1.0817e-04,  3.3448e-06,  ...,  3.5593e-05,\n",
      "         -7.0908e-05, -1.4538e-06],\n",
      "        [ 1.0738e-05,  2.9799e-05,  5.2253e-05,  ..., -1.0827e-04,\n",
      "          7.5915e-06, -1.7391e-06],\n",
      "        ...,\n",
      "        [ 4.8682e-05, -2.1260e-05, -2.7961e-05,  ..., -1.2301e-05,\n",
      "          5.5685e-05,  4.0911e-05],\n",
      "        [-7.9508e-05, -5.9207e-05, -8.0669e-05,  ..., -2.4451e-05,\n",
      "         -2.9262e-05, -1.5113e-05],\n",
      "        [ 9.9600e-05,  6.5730e-05, -1.8621e-05,  ..., -8.3142e-05,\n",
      "         -3.1152e-05, -5.9468e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[2.2992e-07, 2.2308e-07, 2.4020e-07,  ..., 2.5700e-07, 2.3064e-07,\n",
      "         2.2167e-07],\n",
      "        [8.2049e-08, 7.5023e-08, 1.3407e-07,  ..., 8.1621e-08, 6.2508e-08,\n",
      "         6.6757e-08],\n",
      "        [1.7548e-07, 1.5393e-07, 1.6382e-07,  ..., 1.9768e-07, 1.6323e-07,\n",
      "         1.4406e-07],\n",
      "        ...,\n",
      "        [5.8610e-08, 5.4330e-08, 5.3206e-08,  ..., 7.0189e-08, 5.5554e-08,\n",
      "         4.4247e-08],\n",
      "        [7.1379e-08, 6.3716e-08, 7.8463e-08,  ..., 8.5430e-08, 6.6318e-08,\n",
      "         5.4387e-08],\n",
      "        [5.6491e-08, 6.2552e-08, 7.3483e-08,  ..., 6.7695e-08, 5.5793e-08,\n",
      "         5.4349e-08]], device='cuda:0')}, 2472662862208: {'step': 29000, 'exp_avg': tensor([[-3.4109e-05, -7.8870e-06, -9.1917e-06,  ...,  1.0853e-05,\n",
      "          6.9145e-06, -3.3041e-05],\n",
      "        [-1.9160e-05,  1.4807e-05, -1.5643e-05,  ...,  2.2200e-05,\n",
      "         -2.2162e-06, -1.6004e-06],\n",
      "        [ 3.6174e-05,  8.8502e-06, -3.2032e-05,  ...,  1.0970e-05,\n",
      "          5.0614e-05, -1.9119e-05],\n",
      "        ...,\n",
      "        [-2.6625e-06,  1.0553e-05, -2.0709e-05,  ..., -6.5012e-06,\n",
      "         -1.4099e-05,  3.1242e-05],\n",
      "        [ 1.9281e-05,  6.4452e-06,  8.8247e-06,  ..., -5.6273e-06,\n",
      "          5.5506e-05,  2.5080e-05],\n",
      "        [ 5.7082e-05,  1.2062e-06,  2.5852e-05,  ...,  8.1278e-06,\n",
      "         -4.7538e-05,  3.5595e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[2.2389e-08, 2.1127e-08, 2.5252e-08,  ..., 2.2814e-08, 2.8525e-08,\n",
      "         2.1323e-08],\n",
      "        [1.1240e-08, 8.1905e-09, 9.0108e-09,  ..., 1.1866e-08, 1.2635e-08,\n",
      "         1.0140e-08],\n",
      "        [1.6948e-08, 1.3084e-08, 1.4311e-08,  ..., 1.7331e-08, 1.8474e-08,\n",
      "         1.4308e-08],\n",
      "        ...,\n",
      "        [1.0192e-08, 6.6274e-09, 9.0660e-09,  ..., 1.5600e-08, 9.8026e-09,\n",
      "         9.8289e-09],\n",
      "        [1.4822e-08, 1.0903e-08, 1.1645e-08,  ..., 1.3453e-08, 2.7175e-08,\n",
      "         1.1140e-08],\n",
      "        [1.0209e-08, 7.2552e-09, 8.9824e-09,  ..., 1.2791e-08, 1.0809e-08,\n",
      "         2.2412e-08]], device='cuda:0')}, 2473083832384: {'step': 29000, 'exp_avg': tensor([[ 4.0840e-06,  1.7532e-07, -1.3281e-06,  ..., -1.6553e-06,\n",
      "         -6.0411e-06, -1.4891e-05],\n",
      "        [ 3.8508e-06,  3.7223e-06, -2.0702e-06,  ..., -8.8834e-07,\n",
      "         -8.0270e-06,  5.0683e-06],\n",
      "        [-2.8570e-06,  2.9586e-06, -5.4984e-06,  ..., -1.0458e-06,\n",
      "          9.0800e-06,  2.7335e-06],\n",
      "        ...,\n",
      "        [-1.8154e-06, -9.9726e-07,  2.7042e-06,  ..., -1.1248e-06,\n",
      "         -3.0791e-06,  3.2159e-06],\n",
      "        [ 5.7327e-06, -1.7370e-06,  9.9844e-07,  ..., -1.3093e-06,\n",
      "         -2.1925e-05, -1.8722e-05],\n",
      "        [-4.9249e-06, -3.9522e-06,  3.9587e-07,  ..., -1.6820e-06,\n",
      "          1.8997e-06, -1.8317e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[4.3216e-10, 6.0650e-10, 1.6996e-10,  ..., 1.4931e-09, 5.9824e-09,\n",
      "         2.5032e-09],\n",
      "        [7.7304e-10, 5.1816e-10, 2.1270e-10,  ..., 2.5638e-10, 7.2000e-09,\n",
      "         1.7861e-09],\n",
      "        [7.3024e-10, 4.3750e-10, 2.1851e-10,  ..., 3.7679e-10, 5.8690e-09,\n",
      "         1.7688e-09],\n",
      "        ...,\n",
      "        [5.4062e-10, 5.1440e-10, 2.7027e-10,  ..., 3.2825e-10, 5.1080e-09,\n",
      "         4.1725e-09],\n",
      "        [4.2228e-10, 3.6775e-10, 1.6705e-10,  ..., 1.6402e-09, 5.5302e-09,\n",
      "         3.4105e-09],\n",
      "        [9.7807e-10, 9.0873e-10, 1.9255e-10,  ..., 7.4124e-10, 1.1792e-08,\n",
      "         7.2238e-09]], device='cuda:0')}, 2473065124608: {'step': 29000, 'exp_avg': tensor([ 1.0572e-04, -1.4101e-04,  2.4687e-05, -2.9588e-04, -3.0268e-04,\n",
      "        -2.9835e-04, -5.2425e-04, -2.4554e-04, -2.1176e-04, -6.8613e-04,\n",
      "         2.8206e-04, -3.9740e-04,  4.5168e-04,  5.5893e-04, -6.5814e-05,\n",
      "         5.7617e-04,  6.5359e-04,  4.0206e-05,  2.0113e-04, -1.0676e-03,\n",
      "        -4.3045e-04,  5.0820e-04, -3.7637e-04, -2.0811e-04, -6.4806e-04,\n",
      "         6.2146e-04,  3.6370e-04, -1.5312e-04, -9.0067e-04,  2.7129e-04,\n",
      "        -3.7435e-05,  6.4775e-04,  1.6499e-04,  4.6360e-04,  4.4354e-04,\n",
      "        -1.3958e-04, -2.6578e-04, -1.5314e-04,  1.7934e-04, -3.9416e-05,\n",
      "         7.5726e-04, -1.3371e-04, -5.9399e-04, -5.3991e-04, -2.2568e-04,\n",
      "        -3.2189e-04,  2.6722e-04,  2.9899e-04,  3.5023e-04,  9.8703e-04,\n",
      "         3.5797e-04,  1.1748e-04, -3.6863e-04, -5.3578e-05, -1.4251e-04,\n",
      "        -2.4592e-04, -4.0737e-04, -2.4762e-04,  6.0893e-04,  9.6171e-05,\n",
      "         5.4732e-04, -4.1362e-04,  3.6565e-04, -1.5447e-04,  1.7572e-04,\n",
      "         2.5650e-04, -2.1488e-04, -9.3052e-04,  4.3348e-04,  1.7702e-04,\n",
      "        -5.1686e-04, -6.1968e-05,  4.8685e-05, -1.8186e-05,  4.3352e-04,\n",
      "         1.5406e-04,  6.6939e-04, -4.1634e-04,  4.3234e-04,  2.9171e-05,\n",
      "        -4.8163e-04, -8.1112e-04,  1.6996e-04,  5.4013e-04,  2.8758e-04,\n",
      "        -7.7770e-05,  1.2530e-04,  1.8934e-04,  2.3206e-04, -1.1149e-03,\n",
      "        -1.8182e-05, -2.9585e-04,  2.8477e-04, -1.2475e-04,  2.8792e-04,\n",
      "        -5.6579e-04,  6.8028e-04, -1.6113e-04, -1.1741e-04, -3.1072e-04,\n",
      "         3.6575e-04, -2.0332e-04, -5.6203e-04,  6.6233e-05, -2.9319e-04,\n",
      "        -1.6366e-04,  7.0305e-04, -3.9702e-05, -6.0023e-05,  1.2180e-04,\n",
      "        -5.1993e-04,  1.0159e-03,  2.9933e-04,  3.0549e-04, -3.0809e-04,\n",
      "        -1.3721e-04,  3.1240e-04,  6.0155e-04,  8.7690e-05, -2.5130e-04,\n",
      "         1.0927e-03,  2.1447e-04, -8.2652e-04, -3.4986e-04,  8.0355e-04,\n",
      "         4.0280e-04, -8.7741e-04, -3.3342e-04, -7.0724e-04,  3.6445e-04,\n",
      "        -9.4213e-05, -1.7895e-04,  3.3291e-04, -1.6424e-04,  3.8545e-04,\n",
      "        -7.0257e-05,  8.3626e-06, -2.8434e-04, -7.8357e-04,  2.1541e-04,\n",
      "         2.4997e-04,  2.2611e-04,  5.0894e-04, -2.3661e-04,  2.6648e-04,\n",
      "         7.7229e-04, -9.1200e-05,  1.3838e-04,  5.4640e-04,  5.1270e-04,\n",
      "        -2.6497e-04, -3.0637e-04,  4.0782e-04, -5.4821e-04,  3.7601e-04,\n",
      "        -4.2657e-04,  5.0964e-04,  1.2271e-04,  2.0284e-04, -2.0197e-05,\n",
      "        -6.0977e-04,  9.4947e-04,  1.6923e-04, -8.5206e-05, -1.6512e-04,\n",
      "        -2.7427e-04,  1.5712e-04, -1.3819e-04,  7.1621e-04,  3.3166e-04,\n",
      "        -3.8992e-05, -2.9060e-04, -9.5331e-04,  8.5865e-04, -9.4351e-04,\n",
      "        -2.1795e-04, -1.5847e-04,  6.9780e-04,  2.2862e-04, -1.4138e-04,\n",
      "        -9.4527e-04, -2.4596e-04, -5.5647e-05,  1.5811e-05,  7.5205e-05,\n",
      "         6.2221e-04,  1.3250e-03, -2.7847e-04, -1.6800e-04, -6.4991e-04,\n",
      "        -2.6104e-05, -4.6114e-05,  3.6861e-04, -8.2356e-04,  1.4354e-04,\n",
      "         8.7404e-04, -5.0609e-04, -2.9014e-05, -6.6923e-04,  1.9916e-04,\n",
      "         3.4734e-04,  1.8780e-05, -1.5265e-04, -2.4830e-04,  3.7352e-04,\n",
      "        -3.3251e-04, -4.5261e-04,  4.3960e-04,  1.3554e-04, -2.2104e-04,\n",
      "         1.0849e-03, -9.1591e-05,  8.9097e-05, -7.5181e-05,  1.1483e-04,\n",
      "         5.6108e-04, -6.0649e-04,  4.9724e-05, -6.1032e-04,  5.7678e-05,\n",
      "        -1.2137e-04, -3.5609e-04,  1.4160e-04, -3.5360e-04,  2.0711e-04,\n",
      "         4.8175e-04, -1.2943e-04, -3.1566e-04,  2.3215e-04, -3.0108e-04,\n",
      "         2.5978e-04,  5.3725e-04, -8.4197e-05, -2.1374e-04, -1.0944e-04,\n",
      "        -1.9310e-05,  4.0208e-05,  1.0909e-04, -1.5160e-04,  3.1439e-04,\n",
      "         6.6516e-04,  1.9133e-04, -4.4729e-04, -5.9176e-04,  1.0037e-03,\n",
      "        -2.1273e-05, -2.0739e-04,  6.0057e-04,  1.0828e-04,  1.1508e-04,\n",
      "        -1.3761e-05,  3.1522e-04,  4.5467e-04, -2.9334e-04, -1.3770e-04,\n",
      "         7.1638e-06, -3.0980e-04,  8.2451e-04, -3.0669e-04,  1.0170e-03,\n",
      "         3.2722e-04, -4.6202e-04, -2.1102e-04, -3.3523e-04,  2.3083e-04,\n",
      "        -1.1253e-04,  3.1684e-04,  4.3312e-04,  1.4484e-05,  3.8032e-05,\n",
      "        -4.2901e-04,  7.5428e-05,  7.1294e-05, -5.2383e-04, -9.2421e-04,\n",
      "         7.5883e-04,  4.3538e-04,  1.1188e-03, -3.4618e-04, -8.0310e-05,\n",
      "         4.2151e-04,  4.1512e-05,  1.4904e-04,  4.4300e-04, -4.5223e-04,\n",
      "        -7.6999e-04, -2.2152e-05, -3.8883e-04,  5.4324e-05,  1.1526e-05,\n",
      "        -1.3004e-04, -3.7571e-04, -8.2486e-04,  5.1615e-04, -2.8126e-04,\n",
      "         8.3242e-05,  3.3934e-04,  4.9636e-04,  2.6265e-04, -1.9755e-04,\n",
      "        -2.3122e-04, -5.2920e-04, -3.5400e-04,  2.1225e-04, -3.2848e-04,\n",
      "         2.5931e-05,  1.2724e-04,  1.1190e-03,  3.5726e-04, -2.1638e-04,\n",
      "         4.0506e-04, -3.1705e-04,  4.1077e-05,  6.6286e-05,  2.0561e-04,\n",
      "        -6.1868e-04, -3.5533e-04,  4.2466e-05, -4.5005e-04, -1.0823e-03,\n",
      "        -7.5832e-04, -7.9741e-05, -5.7969e-05,  2.1753e-04, -1.2397e-04,\n",
      "         3.3908e-04,  1.1323e-03, -4.8373e-04, -3.7608e-04, -8.5953e-05,\n",
      "         3.1376e-04,  7.5457e-04, -8.3219e-04,  2.1943e-06,  1.2275e-04,\n",
      "         1.6807e-04,  3.0496e-04, -1.0218e-04,  6.5003e-04,  7.4053e-05,\n",
      "         1.8265e-04,  8.4271e-04, -3.3250e-04,  3.1327e-04, -4.9114e-04,\n",
      "         1.0418e-03, -5.4299e-04,  4.8729e-04, -6.0233e-05,  2.9436e-04,\n",
      "        -2.3989e-05, -1.3315e-04,  2.9705e-04,  3.9403e-04, -2.0157e-04,\n",
      "         5.4508e-04,  5.7654e-04, -8.5835e-04,  7.8622e-05, -6.6999e-04,\n",
      "         7.7134e-04, -3.9533e-04,  8.3034e-04, -2.4357e-04, -2.7903e-04,\n",
      "         2.9091e-04, -3.1978e-05,  5.1256e-05,  4.1510e-04, -3.5086e-04,\n",
      "         8.6669e-04,  1.0258e-03, -3.0195e-04, -4.3351e-04,  4.3040e-05,\n",
      "         1.8155e-04,  2.8151e-04,  1.4881e-05,  8.0946e-04,  8.7682e-05,\n",
      "         5.7851e-04,  2.8570e-04,  6.2491e-04,  5.2097e-04, -7.0182e-04,\n",
      "         1.3173e-04, -5.6352e-04,  6.0022e-04,  9.7043e-04,  4.9293e-04,\n",
      "        -6.3565e-05, -4.6395e-04, -5.0258e-04, -2.6945e-04,  2.5439e-04,\n",
      "        -9.6807e-04, -7.7407e-05,  2.0522e-04,  2.4337e-05, -5.5194e-05,\n",
      "         2.3714e-04,  7.6824e-04, -2.9122e-04, -3.8804e-04, -6.9169e-04,\n",
      "         2.5349e-04, -1.9739e-04, -4.0765e-04,  1.5174e-04, -8.8465e-05,\n",
      "         7.5194e-06, -7.3041e-04,  3.2575e-04, -2.0939e-05, -5.0735e-04,\n",
      "         4.4607e-04, -4.6690e-04, -2.6593e-04, -3.9360e-05, -5.5139e-04,\n",
      "         1.5020e-04, -9.4668e-04,  3.5679e-05,  4.4912e-04,  4.5055e-04,\n",
      "         5.1282e-04, -9.5896e-05,  1.2758e-04, -6.8213e-04,  2.7127e-04,\n",
      "        -3.4461e-04,  5.8527e-04,  1.6738e-04,  4.8219e-04,  1.3968e-04,\n",
      "        -1.8532e-04, -7.3626e-05, -5.4531e-04, -6.9088e-05,  5.9301e-04,\n",
      "         2.5084e-04,  2.1232e-04,  4.1050e-04, -8.5236e-04, -5.3963e-05,\n",
      "         1.5803e-04,  4.5545e-04,  4.8957e-04,  6.5307e-05, -6.7996e-04,\n",
      "        -2.0869e-04, -1.8677e-04,  1.0358e-04,  3.6584e-04,  1.5217e-05,\n",
      "        -4.0746e-04,  2.8376e-04,  5.3115e-04, -6.3989e-04, -9.5892e-04,\n",
      "        -8.3105e-05, -2.5202e-04,  1.2236e-04, -1.1708e-04, -5.0754e-04,\n",
      "        -2.9678e-04,  2.5179e-04, -8.7314e-04,  6.1743e-05,  2.6124e-04,\n",
      "         1.0242e-04, -4.9180e-04, -4.4650e-04,  1.9135e-04,  6.3000e-04,\n",
      "         1.7025e-04,  6.2083e-04, -1.6070e-04, -9.7604e-04,  5.8248e-04,\n",
      "         6.2636e-04, -3.9187e-05, -2.5623e-04, -1.3050e-04, -3.7003e-04,\n",
      "         4.4398e-04, -1.0356e-04, -9.8008e-05, -5.7296e-05,  5.9524e-04,\n",
      "         5.5371e-04, -1.4572e-05, -3.7745e-04, -9.0472e-04, -2.5691e-04,\n",
      "        -5.8627e-04,  5.7968e-04,  1.2150e-04,  4.3557e-04,  3.5045e-04,\n",
      "         7.8414e-04,  3.2854e-05, -1.0158e-03, -4.0044e-04,  1.0119e-04,\n",
      "        -1.1244e-03, -3.2164e-04,  1.9572e-05,  5.9999e-04, -1.8825e-04,\n",
      "        -1.2912e-04, -4.3381e-07], device='cuda:0'), 'exp_avg_sq': tensor([3.4071e-06, 3.3802e-06, 3.4110e-06, 3.8292e-06, 3.6847e-06, 3.6106e-06,\n",
      "        3.0632e-06, 4.1538e-06, 3.2664e-06, 3.7316e-06, 3.3822e-06, 3.3789e-06,\n",
      "        3.7644e-06, 3.1838e-06, 4.5167e-06, 8.9732e-07, 3.9599e-06, 3.3746e-06,\n",
      "        3.8380e-06, 4.9755e-06, 3.6264e-06, 3.2945e-06, 3.1964e-06, 4.1718e-06,\n",
      "        3.8772e-06, 2.1161e-06, 3.3544e-06, 3.8418e-06, 3.5463e-06, 3.5363e-06,\n",
      "        3.2762e-06, 4.5631e-06, 3.4323e-06, 4.0033e-06, 5.6421e-06, 3.8204e-06,\n",
      "        3.7224e-06, 3.3818e-06, 5.1088e-06, 5.6979e-06, 3.6121e-06, 3.6709e-06,\n",
      "        3.2219e-06, 3.4495e-06, 3.2887e-06, 3.6506e-06, 4.0062e-06, 3.6917e-06,\n",
      "        5.3086e-06, 3.5919e-06, 3.1882e-06, 3.8718e-06, 3.3618e-06, 4.1290e-06,\n",
      "        3.6274e-06, 3.2733e-06, 3.7986e-06, 3.7120e-06, 4.5748e-06, 3.9201e-06,\n",
      "        3.6284e-06, 3.2076e-06, 5.5076e-06, 3.4108e-06, 3.3180e-06, 3.6284e-06,\n",
      "        3.2011e-06, 4.4876e-06, 3.6216e-06, 3.5811e-06, 3.9556e-06, 3.3447e-06,\n",
      "        3.3941e-06, 3.6548e-06, 4.0059e-06, 5.7781e-06, 3.1976e-06, 3.2448e-06,\n",
      "        3.6628e-06, 3.5999e-06, 4.3860e-06, 3.6653e-06, 3.3451e-06, 4.5291e-06,\n",
      "        6.4694e-06, 3.4814e-06, 4.0849e-06, 4.0060e-06, 4.1863e-06, 3.7045e-06,\n",
      "        3.9389e-06, 3.2271e-06, 5.0282e-06, 3.6776e-06, 3.5705e-06, 3.7770e-06,\n",
      "        3.3218e-06, 3.2209e-06, 4.0814e-06, 3.6361e-06, 3.4563e-06, 3.5696e-06,\n",
      "        3.4597e-06, 3.0424e-06, 3.8165e-06, 3.7706e-06, 3.5893e-06, 4.0797e-06,\n",
      "        3.5126e-06, 3.5552e-06, 3.1190e-06, 4.3561e-06, 4.0807e-06, 4.1379e-06,\n",
      "        3.4504e-06, 3.5515e-06, 3.7113e-06, 3.5492e-06, 3.3225e-06, 4.1835e-06,\n",
      "        4.1381e-06, 3.1880e-06, 3.8461e-06, 3.7067e-06, 4.1542e-06, 4.7630e-06,\n",
      "        3.7663e-06, 4.5506e-06, 3.1757e-06, 3.5384e-06, 3.2631e-06, 4.0762e-06,\n",
      "        2.9782e-06, 3.3968e-06, 3.0923e-06, 3.1343e-06, 3.9848e-06, 3.6674e-06,\n",
      "        3.8441e-06, 3.7328e-06, 3.2711e-06, 5.3064e-06, 3.5539e-06, 3.8926e-06,\n",
      "        4.0048e-06, 3.7684e-06, 3.2704e-06, 2.9443e-06, 5.1107e-06, 3.2454e-06,\n",
      "        4.1670e-06, 4.1998e-06, 4.1539e-06, 3.3947e-06, 4.6518e-06, 3.5399e-06,\n",
      "        3.9556e-06, 3.6733e-06, 2.7474e-06, 4.3106e-06, 1.8661e-06, 3.4896e-06,\n",
      "        3.3027e-06, 3.6219e-06, 3.5413e-06, 3.4444e-06, 3.2521e-06, 3.4492e-06,\n",
      "        3.7674e-06, 4.5408e-06, 3.2424e-06, 3.6560e-06, 4.2198e-06, 3.7914e-06,\n",
      "        3.8676e-06, 3.4118e-06, 3.3270e-06, 3.5982e-06, 3.2613e-06, 3.7717e-06,\n",
      "        3.6821e-06, 3.4718e-06, 3.8661e-06, 3.5401e-06, 3.3424e-06, 3.4649e-06,\n",
      "        3.9594e-06, 3.2735e-06, 4.1673e-06, 3.3516e-06, 4.9399e-06, 3.2781e-06,\n",
      "        3.8677e-06, 4.7975e-06, 3.8279e-06, 3.3715e-06, 3.6266e-06, 3.5558e-06,\n",
      "        3.4958e-06, 4.4490e-06, 3.6108e-06, 3.4627e-06, 3.1722e-06, 3.6075e-06,\n",
      "        3.4308e-06, 3.4005e-06, 3.3547e-06, 3.0890e-06, 3.5853e-06, 3.9678e-06,\n",
      "        3.4151e-06, 3.4588e-06, 3.2958e-06, 3.3131e-06, 3.9617e-06, 3.3169e-06,\n",
      "        4.0606e-06, 3.7024e-06, 3.1928e-06, 3.6952e-06, 3.8295e-06, 3.1569e-06,\n",
      "        3.7164e-06, 3.7124e-06, 3.4861e-06, 3.5124e-06, 5.1384e-06, 3.1952e-06,\n",
      "        3.6045e-06, 3.4219e-06, 4.8309e-06, 3.2686e-06, 3.5392e-06, 3.6985e-06,\n",
      "        3.5773e-06, 3.7114e-06, 3.8806e-06, 3.1873e-06, 3.4695e-06, 3.3479e-06,\n",
      "        3.5626e-06, 3.4660e-06, 3.5904e-06, 3.6330e-06, 3.2389e-06, 3.8735e-06,\n",
      "        4.0471e-06, 3.9724e-06, 3.7168e-06, 3.7701e-06, 3.9345e-06, 3.9197e-06,\n",
      "        3.7727e-06, 3.0015e-06, 3.5174e-06, 3.5115e-06, 4.0406e-06, 3.3450e-06,\n",
      "        3.2910e-06, 3.4680e-06, 4.9923e-06, 3.1248e-06, 4.3426e-06, 3.4777e-06,\n",
      "        3.2794e-06, 3.5789e-06, 3.8696e-06, 3.6792e-06, 3.3844e-06, 4.2268e-06,\n",
      "        2.8286e-06, 3.6401e-06, 3.2649e-06, 5.7818e-07, 3.0921e-06, 3.2359e-06,\n",
      "        3.5203e-06, 3.4701e-06, 3.4514e-06, 3.6976e-06, 4.0010e-06, 3.9451e-06,\n",
      "        3.5401e-06, 3.4839e-06, 2.7468e-06, 3.5745e-06, 3.1517e-06, 3.1360e-06,\n",
      "        3.0604e-06, 3.0380e-06, 3.3198e-06, 3.0846e-06, 4.1196e-06, 3.5925e-06,\n",
      "        4.0627e-06, 3.5334e-06, 3.8156e-06, 3.5068e-06, 3.8138e-06, 3.6270e-06,\n",
      "        3.7494e-06, 4.3489e-06, 3.5726e-06, 3.4729e-06, 4.3712e-06, 3.2174e-06,\n",
      "        3.7416e-06, 3.7541e-06, 3.3845e-06, 3.1782e-06, 3.3916e-06, 3.8418e-06,\n",
      "        4.3363e-06, 3.7227e-06, 3.9353e-06, 3.1521e-06, 3.8421e-06, 3.3159e-06,\n",
      "        3.7230e-06, 3.4869e-06, 3.7791e-06, 5.3301e-06, 3.4598e-06, 3.3269e-06,\n",
      "        4.5649e-06, 3.9320e-06, 3.9210e-06, 3.4364e-06, 3.6528e-06, 3.5676e-06,\n",
      "        3.8904e-06, 3.4575e-06, 3.1865e-06, 5.1996e-06, 3.8760e-06, 3.7120e-06,\n",
      "        3.4535e-06, 5.1898e-06, 3.4314e-06, 3.6791e-06, 3.9396e-06, 3.0667e-06,\n",
      "        3.3177e-06, 3.8242e-06, 3.0690e-06, 3.9519e-06, 3.8263e-06, 4.1887e-06,\n",
      "        3.1329e-06, 5.1202e-06, 3.9280e-06, 3.8803e-06, 3.1808e-06, 3.5341e-06,\n",
      "        3.6285e-06, 3.4464e-06, 4.1027e-06, 3.6924e-06, 3.6697e-06, 3.5910e-06,\n",
      "        3.7318e-06, 3.6235e-06, 3.9548e-06, 3.8370e-06, 4.9386e-07, 3.5319e-06,\n",
      "        4.0957e-06, 3.3993e-06, 3.5103e-06, 3.4727e-06, 6.2240e-06, 3.2751e-06,\n",
      "        5.9740e-06, 3.4994e-06, 3.2191e-06, 5.0646e-06, 3.6414e-06, 3.4913e-06,\n",
      "        3.8748e-06, 3.7776e-06, 3.5983e-06, 3.9687e-06, 4.3784e-06, 3.2627e-06,\n",
      "        3.9568e-06, 3.1081e-06, 2.4729e-06, 3.6700e-06, 3.9295e-06, 3.1378e-06,\n",
      "        3.2137e-06, 3.4879e-06, 3.5951e-06, 3.5164e-06, 3.5106e-06, 3.8064e-06,\n",
      "        3.6868e-06, 3.0914e-06, 3.7842e-06, 3.4272e-06, 3.3156e-06, 3.3406e-06,\n",
      "        3.9360e-06, 3.6081e-06, 3.4419e-06, 3.4500e-06, 5.1369e-06, 3.3265e-06,\n",
      "        4.3035e-06, 4.5457e-06, 3.0574e-06, 3.6462e-06, 3.6083e-06, 3.9433e-06,\n",
      "        4.2353e-06, 3.7976e-06, 3.9994e-06, 3.5812e-06, 3.0533e-06, 3.3628e-06,\n",
      "        4.9583e-06, 3.4377e-06, 3.8029e-06, 4.1750e-06, 4.2423e-06, 3.8805e-06,\n",
      "        3.1389e-06, 4.8526e-06, 3.3820e-06, 3.3454e-06, 4.9531e-06, 4.4013e-06,\n",
      "        3.9798e-06, 3.9101e-06, 3.6987e-06, 3.7525e-06, 3.5830e-06, 3.2778e-06,\n",
      "        4.2089e-06, 3.7268e-06, 3.9501e-06, 3.8883e-06, 3.1308e-06, 4.1406e-06,\n",
      "        3.4088e-06, 4.1011e-06, 3.6917e-06, 5.6736e-06, 3.2443e-06, 3.9706e-06,\n",
      "        3.6278e-06, 3.6954e-06, 3.4317e-06, 3.1060e-06, 2.9964e-06, 3.7893e-06,\n",
      "        3.9965e-06, 3.8103e-06, 3.5732e-06, 4.0958e-06, 3.5942e-06, 3.4330e-06,\n",
      "        3.5712e-06, 3.1659e-06, 4.5419e-06, 3.3121e-06, 4.0360e-06, 3.7424e-06,\n",
      "        4.3226e-06, 5.1745e-06, 4.8295e-06, 3.5984e-06, 3.5273e-06, 4.2903e-06,\n",
      "        3.6137e-06, 3.7569e-06, 4.3891e-06, 3.4535e-06, 4.0352e-06, 3.5275e-06,\n",
      "        3.6016e-06, 4.1163e-06, 3.2268e-06, 3.8502e-06, 3.2991e-06, 5.4829e-06,\n",
      "        3.7008e-06, 3.3776e-06, 4.8359e-06, 3.1320e-06, 4.6790e-06, 3.1546e-06,\n",
      "        3.3497e-06, 4.2672e-06, 3.6781e-06, 3.6411e-06, 3.4314e-06, 3.7849e-06,\n",
      "        3.4125e-06, 4.3663e-06, 3.8120e-06, 3.4516e-06, 3.9749e-06, 3.5772e-06,\n",
      "        3.6042e-06, 3.5150e-06, 3.1244e-06, 3.5284e-06, 3.7701e-06, 3.6319e-06,\n",
      "        3.5788e-06, 3.6118e-06], device='cuda:0')}, 2473084228416: {'step': 29000, 'exp_avg': tensor([[ 7.5750e-05,  6.8813e-05, -3.7315e-04,  ...,  8.7496e-05,\n",
      "          3.6191e-05, -3.1381e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-3.0138e-05,  5.9241e-05,  2.5525e-05,  ..., -7.8490e-05,\n",
      "         -1.4166e-05, -9.4867e-05],\n",
      "        ...,\n",
      "        [-2.5615e-17, -1.2939e-08, -9.6215e-17,  ...,  1.5670e-08,\n",
      "         -4.5511e-17, -2.8025e-08],\n",
      "        [ 1.1818e-09,  4.4864e-13,  2.8979e-09,  ..., -1.7406e-13,\n",
      "          1.8749e-09, -2.0519e-10],\n",
      "        [-1.0754e-05, -3.9659e-06, -9.0880e-08,  ...,  6.5915e-06,\n",
      "         -2.0457e-05, -4.4848e-06]], device='cuda:0'), 'exp_avg_sq': tensor([[3.0721e-07, 3.5838e-07, 4.1818e-07,  ..., 3.3383e-07, 3.6535e-07,\n",
      "         3.2247e-07],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0597e-07, 1.2783e-07, 1.3982e-07,  ..., 2.0705e-07, 1.2618e-07,\n",
      "         1.8430e-07],\n",
      "        ...,\n",
      "        [2.7144e-11, 3.8781e-11, 9.1428e-11,  ..., 9.3083e-11, 2.5688e-11,\n",
      "         1.1865e-10],\n",
      "        [1.0356e-10, 1.3033e-10, 7.4201e-11,  ..., 9.4614e-11, 3.1924e-10,\n",
      "         1.1784e-10],\n",
      "        [1.0575e-10, 8.8957e-11, 2.8825e-11,  ..., 2.2149e-10, 1.0770e-10,\n",
      "         3.8102e-11]], device='cuda:0')}, 2473084227712: {'step': 29000, 'exp_avg': tensor([[-9.3077e-07, -4.1445e-07,  1.0456e-06,  ...,  2.9839e-06,\n",
      "         -1.1284e-05, -3.2753e-06],\n",
      "        [-3.9585e-06,  1.1031e-06,  1.4925e-08,  ...,  1.0011e-06,\n",
      "          1.1021e-05, -1.3002e-05],\n",
      "        [ 1.6896e-08, -1.9170e-08, -2.0090e-09,  ..., -6.5719e-08,\n",
      "          2.7027e-08,  3.3654e-08],\n",
      "        ...,\n",
      "        [ 4.0898e-07,  1.2023e-06,  1.8580e-08,  ...,  7.8480e-06,\n",
      "         -3.4260e-07,  2.5240e-06],\n",
      "        [-9.8847e-07, -3.2507e-07, -1.1969e-06,  ...,  3.3381e-06,\n",
      "         -3.0243e-06,  2.6347e-06],\n",
      "        [-9.0725e-07,  9.4694e-06, -2.1165e-06,  ...,  5.0109e-06,\n",
      "          9.2034e-06, -1.4645e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[6.3206e-12, 7.5143e-12, 9.8592e-12,  ..., 2.3010e-10, 2.2781e-10,\n",
      "         1.9882e-10],\n",
      "        [7.3458e-10, 4.4330e-10, 4.7522e-10,  ..., 8.7947e-10, 3.4698e-09,\n",
      "         1.7388e-09],\n",
      "        [4.9408e-14, 4.7390e-14, 4.9969e-14,  ..., 7.3823e-13, 1.2203e-12,\n",
      "         5.9801e-13],\n",
      "        ...,\n",
      "        [1.6482e-11, 1.7503e-11, 1.6515e-11,  ..., 4.0978e-10, 3.1099e-10,\n",
      "         2.6106e-10],\n",
      "        [2.6918e-10, 1.3453e-10, 1.5932e-10,  ..., 2.3156e-09, 1.7220e-09,\n",
      "         1.2449e-09],\n",
      "        [1.7070e-10, 2.2281e-10, 1.9702e-10,  ..., 4.9215e-09, 6.0792e-09,\n",
      "         3.8527e-09]], device='cuda:0')}, 2473084230848: {'step': 29000, 'exp_avg': tensor([ 2.3784e-06,  4.5329e-05,  9.4693e-08, -1.9240e-05,  9.2597e-07,\n",
      "        -1.0585e-05,  2.0917e-05, -3.5651e-06, -1.8676e-05, -3.7868e-07,\n",
      "         3.3415e-05,  2.9697e-06, -6.0272e-05, -2.9661e-05, -3.2847e-06,\n",
      "        -4.9742e-06, -1.5857e-05,  9.9069e-06,  4.0877e-05, -6.2482e-06,\n",
      "        -4.0684e-05,  3.5055e-05,  1.0882e-07,  7.9961e-06,  2.2397e-05,\n",
      "        -4.4843e-05,  3.1523e-05, -3.2823e-06, -1.8912e-05,  5.1649e-05,\n",
      "        -4.4495e-05, -1.0208e-07,  6.0260e-05,  3.4909e-06,  2.8642e-07,\n",
      "        -1.0636e-05, -1.5114e-05,  2.1926e-05,  2.3835e-09, -1.5435e-05,\n",
      "        -4.4849e-07,  2.7879e-06,  2.1973e-05,  1.0948e-05,  4.3810e-08,\n",
      "         3.5852e-05,  2.9317e-05, -1.3719e-05, -1.6554e-05, -9.2713e-08,\n",
      "         6.5598e-06, -1.2979e-05,  1.4921e-06, -3.8297e-06,  5.5363e-06,\n",
      "        -4.4099e-06, -3.8535e-05, -1.7630e-05,  5.0671e-05,  5.0241e-06,\n",
      "         1.0900e-06, -8.8211e-05,  3.0799e-05, -1.4343e-05, -1.1766e-05,\n",
      "         1.1610e-06, -1.8240e-05,  5.8517e-07,  1.2000e-06, -4.4949e-07,\n",
      "         7.3363e-06, -2.3591e-05, -6.0477e-05,  6.5493e-05, -3.2099e-07,\n",
      "        -4.7127e-05, -1.0893e-05, -2.4993e-06,  2.7277e-05,  6.3918e-06,\n",
      "         6.9781e-05, -1.5345e-05, -4.7501e-07, -6.5559e-06,  9.9353e-07,\n",
      "         2.1430e-08,  8.4373e-05, -1.1619e-05, -6.3942e-06, -4.4010e-08,\n",
      "         5.2567e-05,  2.5956e-07, -1.3642e-05,  1.9172e-06, -5.8811e-07,\n",
      "        -2.0252e-09, -6.7709e-06, -4.9491e-05, -4.7218e-05,  3.8188e-06,\n",
      "        -3.1301e-05,  1.9359e-07,  4.8183e-06,  1.1331e-05,  3.2553e-05,\n",
      "         3.7133e-06,  5.5155e-07,  3.2615e-08,  4.6781e-05,  5.5924e-05,\n",
      "        -5.0933e-06,  8.6120e-07, -1.7220e-05, -7.4631e-07,  5.2350e-07,\n",
      "        -5.4383e-06,  1.5690e-05,  9.7720e-06,  1.6774e-06, -3.2858e-05,\n",
      "         1.4331e-05, -1.4623e-06,  1.5822e-05,  1.5190e-06,  1.6014e-06,\n",
      "         2.1151e-08,  2.4005e-06, -4.5646e-08,  4.5596e-06,  6.8920e-05,\n",
      "         9.2867e-05,  5.7162e-07, -3.3511e-05,  6.2257e-05, -9.4198e-06,\n",
      "         2.2237e-07, -5.8065e-05,  3.1894e-05, -2.8298e-07,  5.4651e-06,\n",
      "         1.5584e-06,  1.2278e-05,  1.0938e-06,  1.2267e-05, -3.8560e-06,\n",
      "         9.6103e-05, -4.4176e-06,  7.3833e-05, -4.5160e-06,  2.5834e-06,\n",
      "        -5.6285e-06,  2.6385e-06, -8.7140e-06,  4.9512e-06, -3.7956e-06,\n",
      "         9.2671e-06,  5.6347e-05, -1.4328e-05,  5.5865e-05, -1.0442e-05,\n",
      "        -1.2867e-05, -2.2049e-07,  1.8136e-05,  4.8553e-05,  1.2010e-05,\n",
      "         8.4867e-07,  2.7544e-05,  8.1930e-07, -3.1910e-05, -2.0016e-05,\n",
      "         1.5288e-05,  3.7488e-05,  1.0909e-05,  2.5614e-05,  4.3624e-05,\n",
      "        -1.3034e-05,  2.9575e-05,  1.6463e-07, -5.0889e-06,  5.2187e-06,\n",
      "         3.4065e-08, -1.1273e-06,  2.3063e-05, -3.3977e-05,  3.3076e-05,\n",
      "         5.3753e-05, -3.5957e-06, -5.6443e-05,  4.6050e-06,  8.1456e-07,\n",
      "         3.8777e-06, -3.0055e-06,  6.0331e-09, -2.7931e-05,  6.6595e-07,\n",
      "         1.7954e-05, -3.4472e-05, -8.1752e-06, -1.5772e-05, -1.1506e-06,\n",
      "         7.8776e-05, -1.9541e-06, -1.7124e-07,  2.1760e-05,  9.9955e-06,\n",
      "        -3.6496e-08, -1.4605e-05,  5.7253e-06,  2.7108e-05, -3.7150e-05,\n",
      "         4.5060e-06,  1.1611e-05, -3.6258e-07,  1.0243e-05, -8.7600e-06,\n",
      "        -5.4940e-05,  3.3041e-06, -3.9849e-06,  4.1407e-05, -4.4561e-05,\n",
      "        -2.1478e-04,  1.5494e-05,  1.7922e-05, -3.9213e-06, -3.4364e-08,\n",
      "        -3.5265e-05, -1.6571e-05, -9.1729e-06, -1.1905e-05,  3.8572e-06,\n",
      "        -1.0496e-04, -1.7104e-05,  1.8204e-04, -1.9747e-06,  8.9888e-05,\n",
      "         1.5065e-05,  1.8446e-06,  5.3626e-05,  6.1584e-07, -2.9489e-07,\n",
      "         2.0962e-05,  9.0536e-06, -1.9888e-05, -3.0634e-07,  1.2881e-05,\n",
      "        -2.5222e-06, -2.2491e-07,  2.8695e-06,  1.5513e-06,  6.3973e-06,\n",
      "         9.6797e-06,  4.3248e-05, -2.1797e-05, -2.4325e-05, -1.1687e-05,\n",
      "        -5.0067e-05, -4.9876e-05,  3.9987e-05,  2.3053e-06,  4.0054e-07,\n",
      "         1.3531e-05, -2.9850e-09, -2.3240e-05,  1.2536e-05, -6.9036e-06,\n",
      "         6.5788e-06,  3.9670e-05,  1.0430e-06, -5.8108e-08,  1.6538e-06,\n",
      "        -8.3866e-06, -2.1945e-05,  3.9446e-05,  7.1028e-06,  7.3420e-07,\n",
      "        -1.6026e-05,  1.9506e-05,  3.7685e-06, -4.7604e-06, -5.6159e-05,\n",
      "        -3.7847e-06,  6.1420e-05,  2.5632e-06, -2.2974e-05, -1.7118e-05,\n",
      "        -2.2175e-06, -2.2496e-08,  2.8937e-09,  1.2542e-05,  2.3024e-05,\n",
      "        -4.7419e-05,  9.8110e-06,  2.2213e-06,  6.6736e-07,  5.5726e-09,\n",
      "         5.4516e-05, -3.8694e-05, -1.4703e-07,  1.0484e-05,  8.8039e-08,\n",
      "         5.4606e-05,  2.9998e-06, -1.0278e-05,  1.4145e-06, -4.3593e-05,\n",
      "        -5.6679e-06,  3.0104e-05,  9.3325e-06,  6.9372e-07, -1.8850e-08,\n",
      "        -1.0263e-05,  1.9221e-05, -1.8150e-05, -6.8051e-06,  2.1895e-05,\n",
      "        -4.0005e-06,  3.0608e-05,  1.0486e-05, -1.1717e-07,  1.2005e-05,\n",
      "        -8.3929e-06,  3.4016e-06,  4.1126e-05, -1.6331e-05, -1.0920e-06,\n",
      "        -1.0150e-05, -2.4274e-07,  6.0785e-06,  5.3887e-07, -8.8069e-07,\n",
      "         5.9451e-05,  2.9415e-05,  5.4688e-06, -1.0246e-06,  9.9730e-06,\n",
      "         1.4669e-05, -1.1727e-05,  1.5739e-05, -4.5574e-06, -3.9379e-07,\n",
      "         1.5501e-06, -5.0208e-06,  1.2166e-05, -4.7807e-07,  5.7323e-06,\n",
      "        -4.3525e-07,  1.0827e-09, -1.7804e-05, -2.9999e-05,  2.0914e-08,\n",
      "        -1.7963e-06, -6.8316e-05,  9.8656e-06,  2.8101e-06, -2.8990e-05,\n",
      "         1.9420e-05,  1.5620e-07, -5.0924e-05,  3.4336e-05, -1.0091e-05,\n",
      "         4.1035e-05,  3.2347e-06, -9.9079e-07, -4.7065e-05, -1.0571e-06,\n",
      "        -1.8270e-06,  5.8654e-06,  5.8145e-06, -5.5159e-05,  5.6458e-08,\n",
      "         9.4513e-06,  3.4935e-05,  6.8995e-06, -1.4848e-05,  3.4348e-06,\n",
      "        -4.5443e-07, -5.4001e-06, -5.9170e-06, -2.5942e-05, -3.2614e-06,\n",
      "        -1.1290e-05,  2.4563e-06, -2.7937e-06, -1.6571e-07, -5.3562e-05,\n",
      "         4.1485e-05,  1.1962e-05,  1.8423e-05,  5.6544e-05, -7.3730e-09,\n",
      "         1.9606e-04,  1.5489e-06, -6.4294e-06,  6.7908e-06,  3.0940e-06,\n",
      "         1.7953e-06, -7.0352e-06, -5.9123e-06, -7.4461e-05,  3.7390e-06,\n",
      "         1.4412e-07,  2.2601e-07, -5.1840e-06,  4.2247e-06,  1.5062e-06,\n",
      "         1.1720e-05, -2.6688e-07, -4.2218e-05,  5.3664e-05,  1.5243e-04,\n",
      "         1.7587e-05,  8.9650e-06, -3.1619e-05, -9.7867e-05, -6.5168e-07,\n",
      "         3.3449e-07, -3.8962e-06, -3.0893e-05,  2.1745e-06,  2.9031e-05,\n",
      "         2.6421e-05,  2.5417e-05, -7.8379e-06,  8.7360e-05,  9.7237e-06,\n",
      "        -3.9882e-06,  1.6297e-07,  7.4219e-06, -2.9733e-06, -1.8787e-05,\n",
      "         2.8246e-05, -1.0245e-06, -1.4634e-05, -4.6564e-07,  6.0358e-05,\n",
      "         5.5907e-06,  6.9355e-07,  3.6745e-05, -3.6176e-07,  1.3723e-05,\n",
      "        -5.6573e-06,  3.4665e-06, -6.7832e-05, -5.1255e-06,  9.4809e-07,\n",
      "        -5.3046e-05,  2.5623e-06,  2.3551e-05, -1.0136e-06,  8.9531e-06,\n",
      "        -1.4551e-05,  5.3349e-06,  6.7429e-06, -1.0526e-05,  2.7473e-07,\n",
      "        -1.5959e-07, -3.3179e-06,  1.1070e-05,  6.5924e-05, -7.0058e-07,\n",
      "         9.6738e-06, -7.4823e-06, -1.0784e-06,  5.1148e-06, -1.8446e-05,\n",
      "        -5.6158e-05,  8.2638e-07,  2.9917e-05, -7.6257e-06, -4.5138e-05,\n",
      "         2.2966e-06,  1.7134e-05, -3.8121e-05,  7.5166e-05,  5.5456e-05,\n",
      "        -3.4599e-05, -1.4198e-04, -2.7637e-05, -1.5743e-05,  8.8164e-06,\n",
      "         7.8734e-06,  1.5272e-06, -5.1442e-05, -1.8852e-06, -5.6833e-06,\n",
      "        -8.3464e-05, -3.0995e-06,  4.5532e-05, -1.1188e-05, -2.7867e-05,\n",
      "        -5.9194e-06,  7.7190e-06,  6.8493e-05,  1.8125e-06,  5.9363e-06,\n",
      "        -7.0639e-07,  1.6522e-05, -3.2069e-07, -2.0162e-06,  2.8138e-05,\n",
      "         4.0114e-07,  3.5321e-05,  6.4159e-06, -1.4815e-07, -3.1058e-06,\n",
      "        -3.0491e-05, -6.5172e-06,  3.1554e-05,  6.9813e-05,  1.0405e-05,\n",
      "         7.0237e-06,  1.3239e-05], device='cuda:0'), 'exp_avg_sq': tensor([1.1599e-09, 6.6180e-08, 8.2139e-12, 1.1703e-08, 2.1132e-10, 5.9880e-09,\n",
      "        1.4783e-08, 6.2363e-09, 7.9800e-09, 2.5913e-10, 1.2490e-08, 3.4041e-09,\n",
      "        1.3446e-07, 1.2725e-08, 8.7787e-10, 1.8475e-09, 1.2458e-08, 8.9888e-09,\n",
      "        1.3428e-08, 1.1976e-08, 1.3743e-08, 5.0012e-09, 5.9389e-11, 1.8131e-08,\n",
      "        9.4594e-09, 3.6911e-08, 2.2798e-08, 6.6482e-11, 6.9283e-10, 7.8271e-09,\n",
      "        2.7106e-08, 2.0189e-09, 2.8726e-08, 2.2682e-09, 2.2294e-10, 7.6783e-09,\n",
      "        5.5756e-09, 2.5774e-08, 4.1754e-12, 1.0510e-08, 7.9892e-11, 7.9447e-10,\n",
      "        1.4450e-07, 1.5617e-08, 4.2468e-10, 3.6103e-09, 4.2545e-09, 1.6676e-08,\n",
      "        1.6671e-09, 6.6302e-11, 1.5474e-09, 1.1797e-08, 4.5584e-09, 1.5515e-08,\n",
      "        3.0667e-09, 5.1793e-09, 7.7421e-08, 8.9786e-09, 1.5548e-08, 7.8172e-09,\n",
      "        1.3127e-08, 2.2857e-07, 3.4409e-08, 1.0617e-08, 2.0224e-08, 3.2906e-09,\n",
      "        6.9980e-09, 1.2378e-10, 8.3154e-09, 1.9924e-10, 1.1962e-09, 1.9482e-08,\n",
      "        3.5963e-08, 5.4368e-08, 1.3473e-11, 5.0096e-09, 2.9348e-08, 1.9965e-08,\n",
      "        1.1990e-08, 4.2374e-09, 1.3046e-08, 1.2719e-08, 1.3953e-09, 4.6749e-10,\n",
      "        3.2133e-10, 1.5855e-10, 2.6084e-08, 8.1914e-09, 6.7454e-09, 3.3167e-10,\n",
      "        4.0016e-08, 7.3636e-09, 1.6625e-08, 2.7594e-08, 7.6069e-10, 1.7665e-12,\n",
      "        2.8137e-09, 2.9391e-08, 1.1408e-08, 1.0602e-08, 9.6197e-09, 3.7221e-12,\n",
      "        3.5344e-09, 1.1703e-08, 1.5900e-07, 1.5049e-09, 3.3733e-10, 4.5572e-09,\n",
      "        2.1426e-08, 7.5092e-08, 9.6735e-09, 6.5000e-09, 2.5160e-08, 1.3931e-09,\n",
      "        1.7604e-10, 1.0662e-08, 1.1631e-08, 2.0687e-08, 5.6971e-10, 3.6461e-09,\n",
      "        8.8858e-09, 1.5610e-09, 3.7306e-09, 4.4231e-09, 9.3513e-09, 7.4121e-12,\n",
      "        2.6939e-09, 2.7062e-10, 3.1563e-09, 2.2821e-08, 1.6510e-07, 3.9750e-10,\n",
      "        1.8778e-08, 7.8837e-08, 2.5228e-08, 1.9010e-10, 4.2797e-08, 6.0557e-09,\n",
      "        1.2962e-10, 5.0072e-09, 2.0780e-09, 1.0801e-07, 1.8357e-08, 7.7252e-09,\n",
      "        9.5181e-10, 2.0522e-07, 3.3228e-08, 2.8879e-08, 4.0290e-09, 7.1746e-10,\n",
      "        1.8801e-08, 1.4855e-09, 1.2407e-08, 2.3038e-08, 4.9282e-10, 4.5466e-09,\n",
      "        3.7580e-08, 9.0141e-09, 1.9693e-08, 4.0780e-09, 1.7548e-09, 1.2190e-08,\n",
      "        1.9914e-08, 4.8675e-08, 3.9400e-10, 2.0835e-11, 5.5007e-09, 1.1976e-10,\n",
      "        8.4926e-08, 5.5711e-09, 1.4573e-08, 2.7104e-08, 1.7251e-08, 7.3140e-09,\n",
      "        2.8742e-08, 1.7761e-09, 8.3365e-08, 6.6280e-10, 8.9587e-09, 5.2925e-09,\n",
      "        1.4271e-11, 3.2579e-09, 1.2352e-08, 1.4102e-08, 7.7814e-08, 1.9443e-08,\n",
      "        7.6015e-09, 4.4298e-08, 1.7796e-09, 8.5211e-10, 2.4232e-09, 6.8766e-09,\n",
      "        9.0041e-15, 1.2820e-08, 2.5461e-09, 7.4684e-09, 2.6323e-08, 1.3753e-08,\n",
      "        6.6106e-09, 6.9287e-09, 1.6908e-07, 4.9628e-10, 2.0106e-11, 7.7784e-09,\n",
      "        1.3672e-07, 2.7924e-12, 1.0794e-08, 1.9516e-08, 1.0081e-08, 2.7797e-08,\n",
      "        2.3927e-09, 1.0642e-08, 5.0037e-12, 5.2218e-09, 1.2663e-08, 1.6172e-07,\n",
      "        2.2101e-08, 1.2529e-09, 7.9828e-09, 1.4810e-08, 3.6229e-07, 2.1494e-08,\n",
      "        5.8709e-09, 8.5039e-09, 1.3702e-10, 8.8363e-08, 1.0601e-08, 1.4064e-08,\n",
      "        7.2252e-09, 4.2413e-08, 1.2775e-07, 2.1724e-08, 3.1305e-07, 1.0734e-08,\n",
      "        2.1568e-08, 9.8545e-09, 8.5263e-11, 5.1825e-08, 5.6466e-11, 4.3492e-11,\n",
      "        3.0589e-08, 5.2785e-09, 4.0523e-09, 9.8444e-10, 5.5916e-09, 5.1361e-10,\n",
      "        1.1765e-09, 9.9083e-09, 2.7843e-08, 2.6569e-09, 9.0835e-09, 2.8725e-08,\n",
      "        7.9690e-09, 8.7365e-08, 3.4536e-09, 3.3635e-08, 1.9148e-08, 1.1639e-08,\n",
      "        1.1210e-08, 2.3985e-11, 1.0165e-08, 5.0736e-15, 2.8985e-08, 1.3716e-08,\n",
      "        1.2895e-08, 6.5220e-08, 6.1835e-08, 4.6422e-09, 1.2095e-09, 1.1142e-08,\n",
      "        7.9947e-09, 1.1470e-08, 6.9279e-09, 5.4448e-08, 1.5567e-10, 4.4203e-09,\n",
      "        1.0021e-08, 1.4768e-09, 1.8170e-09, 8.8741e-08, 1.5656e-09, 3.2686e-08,\n",
      "        2.5782e-08, 3.8841e-08, 3.0167e-08, 2.7849e-09, 2.9085e-13, 2.3512e-12,\n",
      "        3.4666e-09, 2.4101e-08, 6.7990e-09, 5.0534e-09, 6.5991e-09, 2.9441e-09,\n",
      "        4.4017e-12, 3.5651e-08, 4.5260e-08, 1.9324e-11, 4.1899e-09, 6.2978e-08,\n",
      "        1.8520e-08, 1.4950e-09, 1.3643e-08, 4.5006e-11, 6.2207e-08, 1.5752e-08,\n",
      "        1.6237e-08, 1.3494e-08, 6.6386e-10, 6.0275e-09, 1.9573e-09, 1.3062e-08,\n",
      "        1.7518e-09, 1.1845e-09, 1.7303e-08, 6.7292e-09, 4.4674e-08, 2.8922e-08,\n",
      "        3.6411e-09, 2.0813e-10, 9.2289e-10, 8.3722e-09, 1.2188e-08, 5.6675e-09,\n",
      "        1.7538e-08, 2.1809e-08, 1.3576e-10, 8.1718e-09, 2.7331e-10, 2.9619e-10,\n",
      "        1.6586e-08, 4.9010e-08, 1.7489e-09, 1.3693e-08, 1.2036e-08, 1.6372e-08,\n",
      "        8.8998e-09, 4.3209e-09, 6.2695e-09, 1.0051e-09, 2.3735e-10, 2.5530e-09,\n",
      "        1.7265e-09, 6.7204e-09, 2.1703e-08, 5.1700e-12, 4.1720e-12, 2.6949e-09,\n",
      "        1.7015e-08, 9.3749e-10, 5.2761e-11, 4.2340e-08, 1.2173e-08, 2.0545e-10,\n",
      "        5.3673e-09, 9.7370e-09, 1.5490e-10, 3.7297e-08, 1.3601e-08, 6.9282e-10,\n",
      "        3.6141e-08, 5.5839e-08, 5.3636e-11, 9.9824e-08, 8.8524e-09, 4.2466e-09,\n",
      "        3.8796e-08, 6.6838e-09, 1.7008e-08, 1.0062e-11, 1.1320e-08, 1.6247e-08,\n",
      "        2.6118e-09, 2.5081e-09, 1.4947e-09, 9.2875e-11, 1.9784e-10, 5.2434e-09,\n",
      "        2.4618e-08, 2.3773e-09, 2.5081e-08, 1.4907e-10, 4.0351e-10, 7.3204e-11,\n",
      "        1.9758e-08, 3.9726e-08, 1.4941e-08, 2.0804e-08, 1.4924e-07, 1.2053e-11,\n",
      "        2.7360e-07, 1.0541e-09, 1.2961e-08, 2.5871e-10, 3.1646e-08, 6.4399e-09,\n",
      "        1.2027e-08, 7.0130e-09, 1.0171e-07, 8.3141e-10, 2.5422e-11, 1.5611e-08,\n",
      "        2.2152e-10, 8.3188e-09, 4.5869e-09, 2.7540e-10, 7.2946e-10, 2.4641e-07,\n",
      "        9.9214e-09, 4.8154e-08, 1.4179e-08, 1.4665e-08, 3.9493e-08, 1.3204e-08,\n",
      "        4.6615e-10, 1.0363e-08, 4.8465e-09, 1.3866e-07, 3.7421e-10, 5.5926e-09,\n",
      "        2.5770e-09, 4.9828e-09, 5.8494e-09, 4.0687e-08, 7.0357e-09, 3.2503e-11,\n",
      "        4.2527e-11, 5.4840e-09, 7.5149e-08, 2.1106e-09, 9.8662e-09, 1.4322e-11,\n",
      "        3.4562e-09, 2.0507e-11, 7.9433e-08, 6.5013e-09, 2.0329e-08, 3.6756e-08,\n",
      "        2.4040e-08, 5.6208e-09, 5.0761e-09, 1.8741e-10, 9.9417e-09, 9.3591e-11,\n",
      "        1.3497e-08, 7.6736e-08, 1.2320e-09, 9.4168e-10, 5.6654e-09, 2.0516e-08,\n",
      "        6.1233e-08, 2.0532e-09, 1.6260e-08, 1.4643e-08, 1.0363e-09, 5.7465e-12,\n",
      "        1.2750e-09, 8.3863e-08, 1.5286e-08, 6.7529e-10, 7.5865e-09, 1.3531e-08,\n",
      "        1.6758e-09, 4.9037e-10, 1.5237e-08, 1.8413e-08, 6.5457e-09, 5.4582e-09,\n",
      "        2.5095e-09, 1.2173e-07, 4.9150e-10, 6.9468e-09, 1.3161e-09, 2.5136e-08,\n",
      "        8.5033e-08, 2.3638e-08, 8.6238e-08, 2.4834e-07, 6.4311e-08, 5.3362e-10,\n",
      "        7.9734e-09, 1.4777e-08, 2.3135e-08, 8.3608e-10, 5.6734e-09, 2.1882e-07,\n",
      "        1.3345e-09, 3.4603e-08, 1.4791e-09, 8.8464e-09, 1.9179e-08, 5.7346e-10,\n",
      "        2.9255e-08, 3.1548e-10, 5.5535e-09, 4.8560e-10, 3.0580e-09, 3.0034e-11,\n",
      "        3.9074e-09, 3.8155e-08, 3.2847e-09, 4.7228e-08, 8.0791e-10, 9.4980e-11,\n",
      "        4.7488e-09, 1.5850e-08, 6.8925e-09, 2.4983e-08, 4.8549e-08, 2.8075e-09,\n",
      "        1.0323e-08, 2.6520e-08], device='cuda:0')}, 2473084228992: {'step': 29000, 'exp_avg': tensor([[-1.3921e-04, -3.4022e-04, -1.2923e-04,  1.0739e-03, -3.9172e-04,\n",
      "         -1.5757e-04, -1.3594e-03, -3.0196e-04, -6.2758e-04,  7.4479e-06,\n",
      "         -9.2319e-04,  1.9085e-05, -4.4495e-04, -3.1606e-04, -4.2379e-04,\n",
      "          1.1038e-03, -8.6833e-04, -3.0139e-04, -3.9112e-04, -1.0784e-03,\n",
      "          7.1572e-04, -4.0773e-04,  7.6334e-06, -3.1209e-04, -4.0263e-04,\n",
      "         -1.1278e-03,  3.3399e-04, -1.6202e-04, -1.8353e-04, -2.2048e-03,\n",
      "          6.1482e-04,  3.2413e-05, -7.4120e-04,  8.4109e-05, -3.6212e-06,\n",
      "         -1.2027e-03,  8.7505e-04,  4.4574e-04, -1.7438e-06,  3.5308e-04,\n",
      "         -2.4552e-05, -7.4332e-05, -2.7004e-03, -1.4088e-03,  4.6202e-05,\n",
      "          1.2886e-03,  8.1884e-04, -1.4515e-04,  5.0923e-04, -1.2625e-06,\n",
      "          5.5648e-04,  2.3886e-04, -8.0750e-04,  2.5586e-06, -2.3362e-05,\n",
      "         -6.1525e-04,  3.4763e-04,  5.2244e-04, -4.1972e-03,  1.9890e-04,\n",
      "          3.1911e-04,  2.5384e-03, -3.9991e-04,  7.6024e-04, -4.8853e-04,\n",
      "          3.6483e-04, -2.2952e-04,  6.7964e-06, -1.6007e-04,  5.5288e-05,\n",
      "          1.0332e-04, -1.5490e-03,  1.9472e-03,  8.6236e-03,  5.6552e-06,\n",
      "          1.3208e-03,  1.0483e-03, -1.2577e-03, -6.3378e-04,  1.0607e-03,\n",
      "          1.6808e-03, -5.1387e-04, -2.3654e-05,  5.3308e-04, -2.0914e-04,\n",
      "         -3.4700e-04, -2.4932e-03, -1.1684e-03,  8.4640e-04, -9.9412e-07,\n",
      "          2.2460e-04, -6.1182e-04,  1.7550e-04, -2.6739e-04, -2.6189e-04,\n",
      "          1.5194e-07,  2.1423e-04,  1.5753e-03,  2.1074e-03,  1.5019e-04,\n",
      "          1.2407e-03, -7.1477e-06,  3.3311e-05,  1.1461e-03, -5.4694e-03,\n",
      "          4.2027e-04,  3.1472e-05,  1.0866e-03, -6.5731e-04,  2.3381e-03,\n",
      "          2.2316e-04, -6.9515e-05,  5.1958e-05, -1.1936e-04,  1.5445e-04,\n",
      "          6.3926e-04,  8.1106e-04, -1.3860e-03,  3.3291e-05,  4.0702e-04,\n",
      "         -1.8485e-03, -1.2264e-04, -5.7394e-05, -2.3060e-04, -9.0133e-04,\n",
      "          1.4569e-06, -2.1371e-04, -1.0285e-05,  8.2553e-05, -4.8475e-04,\n",
      "         -5.1132e-03,  2.7750e-04,  1.1278e-03, -1.0308e-03, -7.1733e-05,\n",
      "         -5.6516e-04,  3.2859e-04, -9.5608e-04,  1.4179e-04,  3.6308e-04,\n",
      "          1.7231e-03, -6.9307e-04,  4.5250e-04, -2.9207e-04, -4.2327e-04,\n",
      "         -3.6444e-04,  2.8120e-03, -3.6533e-03, -8.3277e-04, -2.2711e-05,\n",
      "          5.8861e-04,  2.4352e-04,  5.0533e-05,  6.8262e-04, -9.2711e-04,\n",
      "          2.2004e-04, -7.5641e-04, -2.3428e-04, -9.8475e-04,  8.4662e-04,\n",
      "         -6.6618e-05, -8.7167e-05, -1.0276e-03, -2.1489e-03,  2.0451e-04,\n",
      "          1.0123e-04, -8.6536e-04,  1.0488e-04,  5.3028e-04, -1.5384e-03,\n",
      "         -9.2940e-04, -4.1733e-04, -1.5422e-03,  1.2908e-04, -2.7314e-04,\n",
      "         -3.6101e-04, -7.9222e-04, -8.2225e-05,  3.5749e-04,  1.1266e-04,\n",
      "         -3.0832e-05,  9.0574e-04,  2.2069e-04, -4.6821e-05,  6.6228e-04,\n",
      "         -9.6260e-04, -6.2518e-04,  1.1069e-03,  1.0748e-04,  1.7319e-04,\n",
      "         -1.2096e-04, -1.2916e-03,  1.9709e-07,  2.6181e-04, -1.4486e-04,\n",
      "         -5.4065e-04,  1.6071e-04, -9.1131e-04,  1.7731e-03, -5.9643e-04,\n",
      "         -1.4843e-03, -1.8390e-04,  8.4033e-06, -1.8711e-03, -3.0686e-04,\n",
      "          6.4094e-06,  1.8734e-03,  9.1127e-04,  5.3552e-04, -1.3108e-03,\n",
      "          1.0779e-04,  8.2209e-04,  8.6290e-06,  1.9563e-03, -1.0630e-04,\n",
      "         -7.4363e-04, -2.7317e-04,  2.5410e-04, -2.8212e-04,  9.9507e-04,\n",
      "         -1.3604e-03, -3.7201e-03,  5.0116e-05, -2.0414e-04,  2.3073e-05,\n",
      "          4.4598e-03, -9.1102e-04,  6.7211e-04,  6.9333e-04, -1.0738e-03,\n",
      "          5.6584e-03,  1.0477e-03, -7.6036e-03,  2.2571e-04, -2.9730e-03,\n",
      "         -1.9708e-04, -9.8326e-05,  2.1085e-03, -1.0150e-04,  9.4780e-06,\n",
      "         -2.0196e-03,  3.3795e-06, -5.6972e-04,  6.1925e-04,  5.9505e-04,\n",
      "         -2.8520e-04, -9.6702e-04,  2.3146e-04,  5.5946e-06,  5.0326e-04,\n",
      "          8.1714e-05, -8.6171e-04, -1.6457e-04, -9.8018e-03,  2.4299e-04,\n",
      "          2.8721e-04,  1.6610e-03, -1.4101e-04,  6.8487e-04,  5.3355e-06,\n",
      "         -1.9272e-05,  7.7351e-08,  7.7991e-05, -2.3126e-03, -2.3599e-03,\n",
      "          2.1519e-03,  3.8673e-04,  1.2216e-03,  5.0753e-04,  1.7586e-04,\n",
      "         -4.7247e-04,  1.8566e-03,  7.2247e-04, -5.6764e-04,  1.4954e-05,\n",
      "         -1.1691e-04, -8.0121e-05,  2.7182e-04, -2.7448e-04,  2.3538e-03,\n",
      "         -2.3026e-04, -1.4458e-03,  1.9964e-03, -6.5634e-05, -4.5062e-04,\n",
      "          3.0668e-04, -6.3248e-07,  4.6355e-08,  9.1275e-04, -6.5719e-04,\n",
      "         -3.5538e-04, -1.5249e-03, -2.4118e-04, -5.9442e-05,  1.0665e-06,\n",
      "          8.1093e-05,  9.6383e-04, -7.1664e-06, -2.0877e-04,  7.2112e-04,\n",
      "         -9.7184e-04,  3.8624e-04,  1.1962e-03,  1.8279e-04,  4.8246e-03,\n",
      "          1.8466e-03,  1.1232e-03, -1.3086e-03,  4.8804e-05, -2.6233e-04,\n",
      "          3.9433e-05, -9.2059e-04, -1.0992e-03, -2.0987e-04,  3.6010e-03,\n",
      "          3.9197e-04,  1.2445e-04,  1.7049e-03, -1.5652e-04,  3.7463e-04,\n",
      "          1.9159e-04,  2.6605e-03, -3.8867e-03, -2.1006e-03, -6.8152e-05,\n",
      "         -4.5466e-05,  3.3132e-06,  2.5888e-04,  3.9012e-04,  7.7211e-04,\n",
      "         -5.6128e-04, -1.2376e-04, -3.3163e-04,  2.4396e-04, -1.7892e-04,\n",
      "         -1.6464e-03,  4.6886e-04, -1.5286e-04,  4.1975e-04, -1.7517e-04,\n",
      "          2.4457e-05, -4.6764e-04, -5.8798e-04, -5.6478e-04, -5.1706e-04,\n",
      "          8.7010e-06,  2.8650e-09, -2.1390e-04,  8.3517e-05,  2.1307e-04,\n",
      "          4.8979e-05,  1.2026e-04,  7.6970e-05, -3.7105e-04, -3.6517e-04,\n",
      "          3.4584e-04, -1.1455e-05,  4.7641e-04, -9.3974e-04, -2.8570e-04,\n",
      "          4.0471e-04, -3.7337e-06, -7.5741e-05,  7.6162e-04, -1.8816e-04,\n",
      "         -3.9496e-04, -1.0973e-03,  3.1422e-04,  1.7449e-03,  6.5036e-06,\n",
      "         -5.3388e-04, -1.9258e-03, -1.8440e-04, -9.2402e-04,  2.4408e-04,\n",
      "         -9.8676e-06, -2.7879e-04,  4.1766e-04, -2.9217e-04, -3.1428e-04,\n",
      "         -1.1402e-04, -4.7798e-05, -2.2025e-04, -2.3453e-04,  4.4742e-03,\n",
      "         -4.4109e-05,  1.4307e-03,  4.7773e-04, -1.4077e-04, -1.1488e-06,\n",
      "         -8.4459e-03,  1.4163e-04,  1.6820e-03,  2.6358e-04, -1.5179e-04,\n",
      "          9.3075e-04, -6.4295e-04, -5.9385e-04,  2.8287e-03,  6.0344e-04,\n",
      "          8.3627e-06,  3.2475e-03,  1.1778e-04, -1.3814e-03,  2.9448e-04,\n",
      "         -1.6915e-04,  1.6697e-04,  2.8163e-05,  1.5742e-03,  2.1579e-03,\n",
      "         -1.1022e-06,  3.7566e-04,  2.3906e-04,  1.1964e-03,  7.0843e-05,\n",
      "          7.3210e-04, -6.2350e-05, -1.8089e-04,  3.3100e-04, -5.3068e-04,\n",
      "         -3.7619e-04, -6.0850e-05,  1.0655e-03, -1.4599e-03,  6.2745e-05,\n",
      "         -5.9590e-04, -2.8675e-06, -1.2941e-03, -3.6913e-03,  8.4108e-04,\n",
      "         -4.1152e-04,  1.8739e-06,  4.6254e-05,  1.0889e-05, -2.8500e-03,\n",
      "          1.1667e-04,  2.1472e-04,  1.2170e-03,  9.6178e-05,  1.8603e-04,\n",
      "         -1.6336e-04,  1.3192e-04,  8.9527e-04, -1.9171e-04, -1.4970e-03,\n",
      "          2.2078e-03,  1.1358e-04,  5.2706e-04, -8.5261e-04,  1.5567e-04,\n",
      "         -2.7539e-03, -2.6989e-04, -7.4573e-05,  3.1947e-04, -1.8984e-04,\n",
      "          1.6191e-06, -1.8657e-04, -1.5695e-03, -4.8908e-04, -9.1003e-05,\n",
      "          2.6701e-04,  1.7404e-03,  2.6620e-04,  2.0051e-05,  3.6383e-04,\n",
      "          3.9849e-03,  7.7276e-04, -6.6356e-04,  1.6598e-04,  5.5729e-04,\n",
      "          1.0377e-04,  2.1175e-04, -5.9966e-04, -9.7004e-04, -3.5150e-03,\n",
      "          1.3072e-03,  1.2366e-03, -2.3097e-03,  1.5364e-04, -5.8736e-05,\n",
      "         -2.0086e-05, -1.7685e-05,  5.8262e-04, -2.7547e-03, -2.3554e-04,\n",
      "         -5.1660e-03, -3.7531e-04, -1.3514e-03,  2.0362e-04,  1.1599e-03,\n",
      "         -5.0196e-04,  4.0658e-04, -1.5755e-03,  3.7572e-05,  8.8171e-04,\n",
      "          1.5302e-06,  1.0300e-04,  6.8700e-06, -3.5304e-04, -9.8845e-04,\n",
      "         -3.7557e-05,  3.0543e-03, -1.1805e-04,  2.8869e-05, -4.7098e-04,\n",
      "          7.8286e-04, -9.2888e-04, -1.4361e-03, -2.3384e-03, -5.5067e-04,\n",
      "         -1.0667e-04,  3.6968e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[1.1114e-05, 1.0834e-06, 4.1178e-08, 1.2675e-04, 2.3262e-06, 7.6175e-06,\n",
      "         1.6023e-04, 4.5538e-06, 6.5863e-06, 3.6857e-07, 2.3239e-05, 7.3036e-06,\n",
      "         1.9395e-04, 4.7278e-06, 3.3546e-06, 9.2201e-06, 2.0521e-05, 1.1165e-05,\n",
      "         4.3970e-05, 2.5159e-05, 1.6822e-05, 7.5430e-06, 4.7733e-07, 5.1169e-05,\n",
      "         1.0388e-05, 8.9795e-05, 6.6867e-05, 2.2877e-07, 5.5794e-07, 2.0064e-05,\n",
      "         1.8178e-05, 3.4130e-07, 3.6569e-05, 7.3815e-06, 1.2612e-07, 1.9646e-05,\n",
      "         2.0883e-05, 2.6676e-05, 3.3391e-08, 4.4835e-06, 6.8186e-07, 4.4393e-06,\n",
      "         2.0752e-04, 4.6711e-05, 3.5416e-06, 8.6931e-06, 5.0009e-06, 5.9119e-06,\n",
      "         2.1441e-06, 1.5148e-07, 1.6393e-05, 3.9554e-06, 1.1693e-05, 9.7028e-05,\n",
      "         1.8053e-06, 5.6367e-06, 7.8392e-05, 1.7522e-05, 4.6535e-05, 1.5540e-05,\n",
      "         1.4422e-05, 5.7839e-04, 3.3176e-06, 3.9395e-05, 1.9172e-05, 1.3203e-05,\n",
      "         2.4687e-05, 1.7881e-06, 1.6687e-06, 8.7046e-08, 5.7758e-06, 2.0842e-04,\n",
      "         1.5034e-04, 6.9938e-04, 2.5064e-08, 5.6508e-06, 6.1154e-05, 3.0223e-05,\n",
      "         3.8266e-06, 3.1506e-05, 1.4846e-05, 2.4019e-05, 2.6981e-05, 3.5339e-06,\n",
      "         8.5847e-07, 2.0477e-06, 2.1445e-05, 1.7349e-05, 2.4248e-05, 1.5299e-07,\n",
      "         3.1020e-04, 7.7095e-05, 1.5561e-04, 2.0227e-06, 3.2647e-06, 2.5127e-08,\n",
      "         2.9398e-06, 5.3824e-05, 7.6017e-06, 6.2024e-06, 2.9157e-05, 1.8280e-08,\n",
      "         1.7594e-06, 2.3679e-05, 4.4119e-04, 3.8150e-06, 1.2078e-06, 1.1223e-04,\n",
      "         7.4153e-06, 2.9470e-04, 1.4907e-05, 4.5075e-06, 1.1708e-05, 2.9401e-05,\n",
      "         1.5544e-06, 4.3158e-05, 8.4844e-05, 1.1999e-05, 5.8887e-07, 5.9196e-07,\n",
      "         1.9086e-05, 1.8878e-06, 1.5967e-05, 3.7138e-06, 3.4127e-05, 6.6621e-08,\n",
      "         2.1800e-05, 4.6491e-06, 1.6836e-06, 2.1965e-05, 4.2862e-04, 8.1297e-07,\n",
      "         2.4222e-05, 2.6376e-05, 3.5375e-06, 2.3326e-05, 1.1236e-05, 1.1481e-05,\n",
      "         1.1611e-06, 8.7783e-06, 1.8279e-05, 2.0826e-05, 8.8597e-06, 1.1693e-05,\n",
      "         1.1929e-05, 9.0425e-06, 2.0538e-04, 9.6804e-05, 1.4735e-05, 2.1712e-06,\n",
      "         3.8773e-05, 1.6147e-05, 1.2233e-05, 3.1154e-04, 1.0131e-05, 2.0587e-06,\n",
      "         1.0532e-05, 5.3605e-06, 1.4740e-05, 6.7505e-06, 1.1774e-05, 2.2081e-05,\n",
      "         1.8545e-05, 9.7289e-05, 4.1901e-06, 9.5225e-08, 8.3154e-06, 6.4112e-07,\n",
      "         8.1561e-06, 1.3157e-05, 8.6841e-06, 6.8656e-05, 2.3133e-05, 4.0174e-06,\n",
      "         3.2950e-06, 6.2327e-06, 1.5333e-04, 4.4298e-06, 3.5317e-05, 3.1358e-06,\n",
      "         4.9472e-07, 6.9703e-06, 4.1324e-06, 1.7524e-05, 6.6946e-05, 3.5149e-05,\n",
      "         2.8103e-05, 8.4368e-06, 1.4853e-06, 1.5499e-06, 3.5923e-06, 2.5973e-05,\n",
      "         1.2678e-10, 1.5043e-06, 9.7380e-06, 1.6114e-05, 1.0170e-05, 1.4463e-05,\n",
      "         9.7169e-06, 6.8927e-06, 1.0549e-04, 1.1568e-05, 4.1643e-08, 4.4805e-05,\n",
      "         1.9658e-04, 4.3715e-08, 3.4178e-05, 4.0365e-05, 5.3596e-06, 6.6964e-05,\n",
      "         5.8571e-06, 8.7828e-06, 3.4026e-07, 2.2624e-05, 7.5564e-06, 9.9683e-04,\n",
      "         1.1817e-05, 2.4622e-06, 8.1396e-07, 4.6095e-06, 1.2594e-03, 4.0202e-04,\n",
      "         7.1464e-06, 5.9768e-06, 7.6414e-07, 1.1969e-03, 3.6909e-05, 2.4362e-05,\n",
      "         8.5605e-06, 1.5523e-04, 1.2746e-04, 8.7815e-05, 1.2885e-03, 4.5514e-06,\n",
      "         2.5473e-05, 5.1142e-05, 1.1249e-06, 9.1583e-05, 9.1776e-07, 1.6185e-07,\n",
      "         3.1572e-05, 6.3073e-06, 2.2814e-06, 9.7441e-06, 5.8329e-06, 8.2955e-07,\n",
      "         2.1246e-05, 1.2684e-05, 7.0985e-06, 2.9588e-06, 3.1300e-06, 1.6032e-05,\n",
      "         6.8873e-06, 5.2967e-04, 2.5629e-05, 4.0070e-06, 1.1176e-05, 1.3072e-05,\n",
      "         2.0346e-05, 1.7134e-08, 7.1828e-06, 3.2232e-12, 1.0882e-06, 9.8538e-05,\n",
      "         4.5247e-05, 2.0790e-04, 4.8139e-05, 3.0589e-05, 4.2841e-06, 8.5986e-06,\n",
      "         1.3371e-05, 4.9008e-05, 1.5913e-05, 8.0866e-06, 9.4018e-07, 2.4432e-06,\n",
      "         1.2541e-05, 4.9487e-06, 6.7213e-06, 4.3372e-05, 2.9903e-06, 3.8440e-05,\n",
      "         4.8052e-05, 1.2995e-05, 6.9205e-06, 6.7760e-06, 2.5880e-09, 1.3388e-07,\n",
      "         4.9531e-06, 8.9135e-06, 2.4601e-05, 1.2372e-05, 2.9148e-06, 1.8992e-05,\n",
      "         9.2021e-08, 2.0040e-05, 2.3907e-05, 4.3421e-08, 1.9778e-06, 1.9467e-05,\n",
      "         5.1254e-05, 8.1828e-06, 4.2536e-05, 6.6505e-07, 4.9461e-04, 5.1630e-05,\n",
      "         6.8711e-05, 7.3662e-05, 2.6816e-06, 7.3391e-06, 1.0083e-05, 2.0035e-05,\n",
      "         3.5353e-06, 5.7993e-06, 3.2030e-05, 1.8430e-05, 5.6201e-05, 8.6222e-05,\n",
      "         3.3562e-06, 1.6622e-06, 6.8202e-06, 5.2092e-05, 2.5591e-04, 2.6421e-05,\n",
      "         4.8384e-06, 1.2688e-05, 1.4356e-06, 4.0845e-06, 1.7674e-06, 8.9246e-07,\n",
      "         1.4403e-05, 4.8224e-06, 4.2279e-06, 1.2697e-05, 3.0655e-05, 4.6879e-05,\n",
      "         5.5983e-05, 3.0581e-06, 9.2718e-06, 1.9433e-06, 9.7551e-07, 1.1822e-05,\n",
      "         1.8490e-06, 7.6304e-06, 6.3808e-05, 6.1126e-08, 1.6567e-08, 2.0202e-06,\n",
      "         4.7210e-05, 6.5541e-06, 1.0120e-07, 1.1237e-05, 6.2007e-06, 2.4435e-06,\n",
      "         4.7394e-06, 1.8542e-05, 1.3017e-06, 3.5299e-06, 1.6949e-05, 1.0013e-06,\n",
      "         9.1265e-05, 1.5960e-06, 3.3571e-07, 2.0092e-05, 3.1614e-05, 9.3656e-06,\n",
      "         6.6592e-05, 1.3995e-05, 2.0649e-05, 7.0282e-08, 2.5452e-05, 3.3164e-05,\n",
      "         1.2113e-05, 5.1087e-06, 4.1943e-06, 9.6996e-07, 3.2197e-06, 3.4113e-06,\n",
      "         4.5385e-05, 3.5690e-06, 1.1515e-05, 4.1108e-07, 1.4705e-06, 1.7362e-07,\n",
      "         9.1457e-05, 2.2552e-06, 3.2134e-05, 1.5887e-05, 1.0020e-05, 5.1534e-08,\n",
      "         1.0579e-03, 1.8693e-06, 1.3169e-04, 2.3907e-06, 2.3715e-06, 1.8153e-05,\n",
      "         8.1026e-06, 2.3747e-05, 2.5741e-04, 2.5869e-06, 9.3498e-08, 2.6502e-04,\n",
      "         6.4064e-07, 1.1624e-05, 2.7116e-05, 7.0349e-07, 6.8762e-06, 2.5893e-06,\n",
      "         6.1942e-05, 7.2568e-04, 2.0500e-05, 9.9458e-06, 6.4708e-06, 2.2747e-05,\n",
      "         6.0395e-06, 2.8399e-05, 6.7828e-06, 1.1636e-04, 1.2152e-06, 4.8913e-06,\n",
      "         1.2383e-06, 9.0139e-07, 2.1958e-05, 1.9922e-05, 1.8663e-05, 3.2856e-07,\n",
      "         3.2565e-08, 8.9448e-06, 3.3597e-04, 5.1532e-06, 1.4742e-06, 1.0385e-07,\n",
      "         1.6292e-06, 1.0717e-07, 3.7902e-04, 1.2875e-05, 4.3707e-05, 4.3216e-04,\n",
      "         4.5373e-06, 1.4243e-06, 9.3722e-06, 2.6039e-07, 9.9438e-06, 4.7455e-07,\n",
      "         2.0684e-04, 6.4974e-04, 2.3222e-05, 7.1974e-07, 1.8096e-05, 9.0624e-06,\n",
      "         5.1646e-04, 9.9555e-06, 4.3882e-06, 2.8472e-05, 5.9985e-06, 6.4460e-09,\n",
      "         6.8904e-07, 9.6474e-05, 4.0630e-06, 5.7050e-06, 4.1311e-06, 7.4711e-05,\n",
      "         7.7846e-06, 1.0046e-06, 7.3967e-06, 4.6695e-05, 2.8572e-05, 5.9077e-06,\n",
      "         1.0277e-06, 8.8108e-06, 1.7806e-06, 8.4328e-06, 3.6778e-06, 6.6891e-06,\n",
      "         3.6239e-04, 5.7144e-05, 2.3860e-05, 4.9864e-04, 8.7641e-05, 7.4110e-07,\n",
      "         9.4436e-06, 1.5087e-06, 7.3621e-06, 3.5027e-05, 4.8856e-06, 3.9242e-04,\n",
      "         2.1654e-06, 1.5492e-05, 1.1337e-05, 1.3122e-05, 1.1742e-04, 3.2816e-06,\n",
      "         5.8282e-05, 3.5618e-07, 1.6901e-05, 3.0391e-06, 4.5442e-06, 7.6419e-07,\n",
      "         3.6719e-06, 2.4769e-05, 3.1637e-06, 6.4025e-04, 3.6093e-06, 8.3021e-07,\n",
      "         1.9264e-05, 3.1361e-05, 1.1680e-05, 1.3541e-05, 1.1717e-04, 6.7089e-06,\n",
      "         3.1839e-06, 4.7723e-05]], device='cuda:0')}, 2473084227968: {'step': 29000, 'exp_avg': tensor([[ 7.9952e-05,  6.5498e-05, -7.1659e-05,  ..., -7.7114e-06,\n",
      "          5.7379e-06, -2.2919e-05],\n",
      "        [ 1.5905e-05, -2.7929e-05, -3.3970e-05,  ..., -5.5463e-06,\n",
      "          5.5481e-06,  3.9530e-06],\n",
      "        [-4.9535e-06,  3.0072e-06,  1.4977e-05,  ..., -1.2799e-05,\n",
      "         -9.7947e-06,  2.2189e-05],\n",
      "        ...,\n",
      "        [ 2.9848e-05,  2.9025e-06,  4.1794e-06,  ..., -1.5617e-06,\n",
      "         -7.1919e-06, -8.9106e-06],\n",
      "        [-1.5107e-05,  2.7365e-06,  3.0203e-05,  ...,  2.0773e-06,\n",
      "         -8.2903e-08, -6.0653e-06],\n",
      "        [-1.5791e-05, -1.1525e-05, -5.6334e-05,  ...,  2.6201e-07,\n",
      "         -9.5144e-06,  8.0306e-06]], device='cuda:0'), 'exp_avg_sq': tensor([[3.5484e-08, 3.3690e-08, 3.0956e-08,  ..., 3.5804e-09, 2.1306e-09,\n",
      "         3.2181e-09],\n",
      "        [4.6009e-08, 4.0089e-08, 5.0432e-08,  ..., 3.1050e-09, 4.1473e-09,\n",
      "         2.8908e-09],\n",
      "        [3.2974e-08, 2.0492e-08, 1.8753e-08,  ..., 3.0159e-09, 2.1927e-09,\n",
      "         1.9089e-09],\n",
      "        ...,\n",
      "        [9.3216e-08, 7.1623e-08, 6.8658e-08,  ..., 5.7395e-09, 6.8548e-09,\n",
      "         5.6861e-09],\n",
      "        [8.6166e-08, 6.9080e-08, 6.0690e-08,  ..., 6.7745e-09, 6.7355e-09,\n",
      "         6.0563e-09],\n",
      "        [1.1046e-07, 8.9915e-08, 6.9915e-08,  ..., 9.1221e-09, 8.5682e-09,\n",
      "         8.5745e-09]], device='cuda:0')}, 2473084228096: {'step': 29000, 'exp_avg': tensor([[ 5.0747e-06, -3.7785e-06,  5.5202e-06,  ...,  9.0865e-07,\n",
      "          1.1974e-05, -1.6655e-06],\n",
      "        [-6.3928e-06,  1.0846e-05, -1.0630e-05,  ..., -4.1886e-06,\n",
      "         -1.4800e-05,  7.2476e-06],\n",
      "        [ 4.9641e-06,  1.7382e-07,  1.0768e-06,  ...,  1.7204e-06,\n",
      "         -2.8558e-06,  4.5339e-06],\n",
      "        ...,\n",
      "        [-1.2277e-05,  5.9603e-07, -5.6992e-07,  ...,  1.2768e-06,\n",
      "         -2.2198e-05, -1.3184e-07],\n",
      "        [ 4.5985e-06,  1.3511e-06,  3.7997e-07,  ...,  6.3503e-06,\n",
      "         -1.0653e-05,  4.4025e-06],\n",
      "        [ 1.0691e-05, -1.5223e-06, -1.4422e-05,  ..., -8.9342e-06,\n",
      "         -2.0032e-05, -5.6551e-06]], device='cuda:0'), 'exp_avg_sq': tensor([[5.6808e-10, 6.0482e-10, 4.2310e-10,  ..., 5.1528e-10, 7.7065e-10,\n",
      "         5.5681e-10],\n",
      "        [6.7058e-10, 6.7856e-10, 7.0556e-10,  ..., 7.8991e-10, 9.2428e-10,\n",
      "         7.9654e-10],\n",
      "        [2.0817e-10, 2.6500e-10, 2.4083e-10,  ..., 2.9681e-10, 2.9163e-10,\n",
      "         2.5075e-10],\n",
      "        ...,\n",
      "        [6.3487e-10, 7.3897e-10, 6.8377e-10,  ..., 6.9339e-10, 8.8883e-10,\n",
      "         7.7710e-10],\n",
      "        [1.0809e-09, 1.1421e-09, 9.9300e-10,  ..., 1.1844e-09, 1.4092e-09,\n",
      "         1.1769e-09],\n",
      "        [1.2338e-09, 1.2525e-09, 1.1578e-09,  ..., 1.5057e-09, 1.4267e-09,\n",
      "         1.4279e-09]], device='cuda:0')}, 2473084229376: {'step': 29000, 'exp_avg': tensor([-6.3867e-05,  1.3620e-04, -8.8642e-07,  ...,  1.2967e-04,\n",
      "        -4.3869e-05,  1.4618e-04], device='cuda:0'), 'exp_avg_sq': tensor([4.4822e-08, 5.3208e-08, 3.6983e-08,  ..., 8.9032e-08, 9.9869e-08,\n",
      "        1.2852e-07], device='cuda:0')}, 2473084229440: {'step': 29000, 'exp_avg': tensor([-6.3867e-05,  1.3620e-04, -8.8642e-07,  ...,  1.2967e-04,\n",
      "        -4.3869e-05,  1.4618e-04], device='cuda:0'), 'exp_avg_sq': tensor([4.4822e-08, 5.3208e-08, 3.6983e-08,  ..., 8.9032e-08, 9.9869e-08,\n",
      "        1.2852e-07], device='cuda:0')}, 2473084230144: {'step': 29000, 'exp_avg': tensor([[-6.6464e-05,  7.2272e-06,  4.7129e-05,  ...,  4.3072e-04,\n",
      "         -4.3380e-04, -6.4710e-04],\n",
      "        [ 4.3571e-05,  1.2850e-04,  3.7010e-05,  ..., -2.9148e-04,\n",
      "         -5.7959e-04,  7.5578e-04],\n",
      "        [ 1.2347e-06,  1.1158e-04,  1.3311e-05,  ..., -8.9006e-04,\n",
      "         -1.9966e-04, -1.7759e-04],\n",
      "        ...,\n",
      "        [ 3.6847e-05, -2.4487e-05, -7.3549e-05,  ...,  8.5846e-05,\n",
      "          4.2485e-04,  7.2487e-05],\n",
      "        [ 7.9488e-05,  6.7200e-05,  1.0420e-04,  ...,  2.6719e-05,\n",
      "          2.0854e-05, -2.7165e-05],\n",
      "        [ 1.0365e-04,  6.9200e-05, -4.4947e-05,  ..., -2.3600e-04,\n",
      "         -1.1972e-04,  4.5133e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[9.3805e-08, 9.9943e-08, 1.3362e-07,  ..., 1.8247e-06, 1.9175e-06,\n",
      "         1.8986e-06],\n",
      "        [8.9873e-08, 1.0078e-07, 8.8339e-08,  ..., 1.9622e-06, 1.9937e-06,\n",
      "         1.8894e-06],\n",
      "        [8.7992e-08, 9.5247e-08, 9.1471e-08,  ..., 1.7886e-06, 1.8893e-06,\n",
      "         1.7338e-06],\n",
      "        ...,\n",
      "        [1.0728e-07, 1.0989e-07, 1.0354e-07,  ..., 2.1455e-06, 2.2883e-06,\n",
      "         2.1518e-06],\n",
      "        [9.3640e-08, 9.0158e-08, 8.8246e-08,  ..., 1.9202e-06, 2.0430e-06,\n",
      "         1.8722e-06],\n",
      "        [1.0495e-07, 8.9951e-08, 7.7506e-08,  ..., 2.0792e-06, 2.2397e-06,\n",
      "         1.9888e-06]], device='cuda:0')}, 2472662805248: {'step': 29000, 'exp_avg': tensor([ 4.4278e-04, -2.6065e-04,  1.1977e-04, -4.0861e-04,  5.6237e-04,\n",
      "        -2.4418e-04, -2.2418e-04,  8.3152e-05,  3.8917e-04, -3.9342e-05,\n",
      "        -2.1617e-04, -4.4999e-04,  5.1543e-04,  2.3264e-05,  4.2982e-04,\n",
      "        -1.8022e-04, -2.1382e-05, -2.6337e-04,  2.6902e-05,  2.2460e-04,\n",
      "        -5.2143e-04, -7.5138e-04, -3.9479e-04,  9.2797e-05,  1.9182e-04,\n",
      "         5.8780e-04, -1.0474e-05, -4.7411e-04, -5.6087e-04,  2.1152e-04,\n",
      "         9.9718e-05,  1.3384e-04,  1.0161e-03,  4.4247e-04, -6.6422e-04,\n",
      "         2.6490e-04, -2.9704e-04,  4.1598e-05, -5.2742e-04, -9.4635e-05,\n",
      "         2.0530e-04, -2.1157e-04,  3.7832e-04, -3.0011e-04, -1.0008e-04,\n",
      "         1.6867e-04,  2.5250e-04,  3.3472e-04, -9.4382e-05, -4.3946e-04,\n",
      "        -1.9702e-04,  4.3958e-04, -2.3100e-04, -4.3069e-04,  8.0730e-05,\n",
      "         1.7347e-04, -3.0250e-04, -2.5467e-05,  6.5086e-04, -2.9737e-04,\n",
      "        -1.8929e-04,  4.8625e-04, -7.4953e-05, -2.1271e-04, -8.9789e-04,\n",
      "         1.7186e-04, -3.2557e-04,  1.3439e-04, -5.4271e-04, -3.9688e-04,\n",
      "        -3.0145e-04,  1.6508e-05, -9.9666e-05, -7.6741e-04,  1.4672e-04,\n",
      "         2.8365e-04, -5.3215e-04,  3.0659e-05,  5.2874e-04, -2.2413e-04,\n",
      "         1.5676e-05, -2.7277e-05, -2.2485e-04, -8.2044e-04, -1.6893e-05,\n",
      "        -5.7994e-04, -5.6342e-04, -1.2277e-04, -2.5705e-04,  2.9841e-04,\n",
      "         1.8581e-04,  8.7975e-05, -3.3427e-04, -6.5919e-05, -1.9603e-04,\n",
      "         4.6387e-04,  7.8862e-04,  8.5073e-05,  6.9739e-06, -4.3095e-04,\n",
      "         4.7209e-04, -2.0082e-04, -2.4171e-04, -4.5372e-04, -1.0674e-04,\n",
      "         3.7172e-04,  2.7982e-04,  3.8093e-04,  5.0415e-05,  6.8126e-04,\n",
      "        -1.0205e-04, -3.4199e-04, -5.9470e-04,  2.4583e-04, -3.0202e-04,\n",
      "        -4.7530e-04, -5.0392e-04, -4.5215e-04,  5.8716e-04,  1.3428e-04,\n",
      "         7.9195e-05, -1.6328e-04, -3.5477e-04,  6.4903e-05,  5.1670e-04,\n",
      "        -4.0988e-04,  7.0844e-04,  8.5984e-05,  2.3598e-04,  4.5867e-05,\n",
      "         1.5964e-04,  8.3681e-05,  3.3319e-04,  1.1246e-04, -1.5367e-04,\n",
      "         7.4232e-04, -8.0853e-05,  7.8364e-05, -5.2469e-04,  6.7409e-05,\n",
      "         8.3073e-05, -9.3787e-07, -3.0529e-05, -5.9216e-04, -4.7695e-04,\n",
      "         1.2457e-04,  6.6509e-04, -1.5504e-04, -1.3074e-04, -1.7872e-04,\n",
      "         4.3466e-05,  1.0449e-06, -5.5977e-04, -6.4488e-05,  3.8305e-04,\n",
      "        -5.6691e-05,  1.0106e-04, -3.9279e-04,  3.2279e-04,  4.3165e-04,\n",
      "         3.7722e-04,  5.4486e-05, -9.6979e-04, -7.5328e-04, -2.1211e-05,\n",
      "         1.2569e-04,  2.2718e-04, -7.7468e-04, -1.8980e-04, -3.3073e-04,\n",
      "         3.1646e-04, -1.6231e-05,  3.0061e-04, -3.4736e-04,  1.8387e-04,\n",
      "         1.1416e-04,  3.3929e-05, -4.6534e-04,  1.8844e-04, -1.5810e-04,\n",
      "        -5.1439e-05,  3.5873e-04,  1.5799e-04,  1.6926e-05,  3.1493e-04,\n",
      "         1.0876e-04,  4.8471e-05,  5.6903e-04, -2.9999e-04, -5.2946e-05,\n",
      "         3.8485e-05, -7.5096e-05,  1.0202e-05, -8.7616e-04,  1.8497e-04,\n",
      "        -6.6736e-04, -7.8397e-05, -1.9135e-04,  7.2518e-04, -3.2809e-05,\n",
      "        -7.9147e-05, -9.0131e-05,  5.6058e-05,  7.4850e-05, -2.5121e-04,\n",
      "         1.0769e-04,  1.1281e-04, -3.2816e-04, -2.2976e-04,  7.7664e-05,\n",
      "        -2.7848e-04,  4.1286e-04, -4.8685e-04,  3.8274e-04, -1.9722e-04,\n",
      "         1.6973e-04, -1.0352e-03,  4.4861e-05, -1.6861e-06,  5.1329e-04,\n",
      "         2.0742e-04,  1.7999e-04, -6.5590e-05,  1.9931e-04, -2.0554e-04,\n",
      "        -2.2247e-04,  6.4137e-04, -2.3940e-04, -7.8776e-05,  2.6501e-04,\n",
      "        -2.0604e-04, -1.0728e-04, -1.3768e-04, -1.5986e-04,  2.5683e-04,\n",
      "        -7.8246e-04,  1.7949e-04,  6.5803e-04, -6.6732e-04,  3.4069e-04,\n",
      "         1.7534e-04, -2.9937e-05, -5.8432e-05,  1.0831e-04, -1.8658e-04,\n",
      "         1.5252e-04, -9.8544e-05,  5.2035e-04,  6.2980e-05,  2.1331e-04,\n",
      "         1.9963e-04,  5.8832e-04,  7.4171e-05, -2.3387e-05,  2.3384e-04,\n",
      "        -7.2486e-05], device='cuda:0'), 'exp_avg_sq': tensor([3.3102e-06, 2.8858e-06, 2.8396e-06, 2.7624e-06, 2.9425e-06, 2.2652e-06,\n",
      "        7.9663e-07, 1.6115e-06, 2.1927e-06, 3.5107e-06, 3.0728e-06, 3.0164e-06,\n",
      "        2.6645e-06, 2.9687e-06, 2.7151e-06, 1.2830e-06, 3.2705e-06, 2.0741e-06,\n",
      "        3.7586e-06, 2.1986e-06, 2.8986e-06, 3.2246e-06, 1.2896e-06, 2.9569e-06,\n",
      "        4.1816e-06, 2.9931e-06, 3.2017e-06, 2.4400e-06, 3.5961e-06, 3.5392e-06,\n",
      "        4.1309e-07, 2.8561e-06, 2.7964e-06, 3.4762e-06, 3.5336e-06, 3.8243e-06,\n",
      "        3.7041e-06, 2.8557e-06, 3.3372e-06, 2.9996e-06, 3.2725e-07, 3.1307e-06,\n",
      "        2.5816e-06, 3.1432e-06, 3.2037e-06, 3.3167e-06, 2.2794e-06, 2.8947e-06,\n",
      "        3.1253e-06, 1.8740e-06, 3.8739e-06, 2.7425e-06, 1.4639e-06, 3.2961e-06,\n",
      "        2.4997e-06, 3.1656e-06, 3.0602e-06, 2.6268e-06, 3.5163e-06, 4.6983e-06,\n",
      "        3.6045e-06, 2.7305e-06, 3.1444e-06, 3.7097e-06, 3.5796e-06, 2.2832e-06,\n",
      "        3.1889e-06, 2.6792e-06, 3.4188e-06, 2.8258e-06, 3.1124e-06, 1.9593e-06,\n",
      "        2.4165e-06, 2.6503e-06, 3.3688e-06, 3.4660e-06, 3.1082e-06, 1.0396e-06,\n",
      "        2.7632e-06, 3.4603e-06, 3.0545e-06, 3.7914e-06, 3.0565e-06, 3.3169e-06,\n",
      "        2.6296e-06, 3.1219e-06, 3.2112e-06, 2.1147e-06, 2.8160e-06, 2.8531e-06,\n",
      "        2.1768e-06, 9.9196e-07, 3.3345e-06, 2.2723e-06, 2.9754e-06, 3.1808e-06,\n",
      "        3.3225e-06, 1.3472e-06, 2.2497e-06, 3.1944e-06, 2.8467e-06, 2.7592e-06,\n",
      "        3.0918e-06, 2.8652e-06, 1.3406e-06, 3.0281e-06, 3.3515e-06, 3.4736e-06,\n",
      "        1.3186e-06, 3.3153e-06, 3.1852e-06, 3.1279e-06, 2.9323e-06, 3.4379e-06,\n",
      "        3.0967e-06, 3.8495e-06, 3.2251e-06, 2.5263e-06, 3.0320e-06, 3.1846e-06,\n",
      "        2.7240e-06, 2.6664e-06, 2.2071e-06, 2.8619e-06, 3.4827e-06, 3.1459e-06,\n",
      "        3.9271e-06, 2.8739e-06, 3.4298e-06, 2.3687e-06, 2.6418e-06, 3.7054e-06,\n",
      "        3.0047e-06, 3.3859e-06, 3.4477e-06, 2.5357e-06, 2.9597e-06, 1.7979e-06,\n",
      "        2.4764e-06, 2.9751e-06, 2.9709e-06, 8.9392e-07, 3.1277e-06, 2.9483e-06,\n",
      "        3.6461e-06, 3.8684e-06, 2.7575e-06, 2.5928e-06, 3.0851e-06, 3.1072e-06,\n",
      "        2.8069e-06, 6.1933e-07, 3.3258e-06, 3.0194e-06, 1.9764e-06, 2.9477e-06,\n",
      "        2.8622e-06, 2.9379e-06, 3.1536e-06, 3.8800e-06, 1.9753e-06, 3.5539e-06,\n",
      "        3.4079e-06, 3.1078e-06, 3.1601e-06, 2.3986e-06, 2.7255e-06, 2.8020e-06,\n",
      "        1.1292e-06, 4.3864e-06, 3.4935e-06, 2.4452e-06, 2.6811e-06, 2.7097e-06,\n",
      "        2.9493e-06, 3.0833e-06, 2.8029e-06, 3.2587e-06, 2.9580e-06, 2.9787e-06,\n",
      "        3.1985e-06, 2.5956e-06, 2.2704e-06, 2.3709e-06, 2.7772e-06, 3.0012e-06,\n",
      "        3.1625e-06, 2.7440e-06, 2.9811e-06, 2.8749e-06, 3.0868e-06, 3.3739e-06,\n",
      "        3.1532e-06, 3.8255e-06, 2.3211e-06, 3.7706e-06, 3.3067e-06, 2.7055e-06,\n",
      "        3.0216e-06, 2.8012e-06, 3.1541e-06, 2.5029e-07, 3.0569e-06, 3.5257e-06,\n",
      "        3.1228e-06, 3.4922e-06, 4.1123e-06, 2.8754e-06, 1.6571e-06, 2.1328e-06,\n",
      "        2.0031e-06, 2.7325e-06, 3.3309e-06, 3.1870e-06, 3.0666e-06, 3.3448e-06,\n",
      "        3.7418e-06, 2.8691e-06, 2.5568e-06, 3.0492e-06, 2.6614e-06, 5.8771e-07,\n",
      "        1.2066e-06, 2.2804e-06, 2.5618e-06, 3.4528e-06, 3.6312e-06, 3.1508e-06,\n",
      "        3.2675e-06, 3.3954e-06, 3.4660e-07, 3.4151e-06, 3.5440e-06, 3.9976e-06,\n",
      "        3.0914e-06, 3.3952e-06, 2.8640e-06, 2.7735e-06, 3.3570e-06, 3.2768e-06,\n",
      "        3.8223e-06, 9.6498e-07, 3.4547e-06, 8.6822e-07, 2.0101e-06, 3.3291e-06,\n",
      "        2.6694e-06, 3.2140e-06, 7.1614e-07, 4.0673e-06, 3.2451e-06, 2.9025e-06,\n",
      "        1.7448e-06, 3.7952e-06, 3.1044e-06, 3.5196e-06], device='cuda:0')}, 2472662804928: {'step': 29000, 'exp_avg': tensor([[ 5.2758e-04, -1.2503e-03,  8.3996e-04,  ...,  4.9367e-05,\n",
      "         -2.0660e-03, -1.0350e-04],\n",
      "        [-1.6632e-12, -1.8340e-12,  1.5372e-12,  ...,  3.5000e-13,\n",
      "          1.4717e-12, -3.8116e-12],\n",
      "        [-2.1670e-12, -2.2985e-12,  9.6460e-13,  ...,  2.8098e-13,\n",
      "          8.9794e-14, -5.0344e-12],\n",
      "        ...,\n",
      "        [ 3.9293e-06,  2.6665e-06,  2.0384e-06,  ...,  1.0896e-06,\n",
      "          1.7786e-06, -2.7384e-06],\n",
      "        [-1.0362e-07, -7.0880e-08,  5.4584e-09,  ..., -1.3313e-10,\n",
      "         -6.2875e-08, -7.8761e-08],\n",
      "        [-5.5442e-05,  6.1164e-05,  3.7575e-05,  ...,  5.7905e-05,\n",
      "          5.3958e-05,  6.2619e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[1.2316e-05, 1.1876e-05, 1.1582e-05,  ..., 1.1796e-05, 1.1739e-05,\n",
      "         1.2825e-05],\n",
      "        [3.7038e-22, 3.8582e-22, 1.2085e-22,  ..., 3.7887e-22, 3.7348e-22,\n",
      "         7.9813e-23],\n",
      "        [7.8868e-22, 8.2607e-22, 2.1223e-22,  ..., 7.9586e-22, 7.9144e-22,\n",
      "         1.3691e-22],\n",
      "        ...,\n",
      "        [5.2778e-09, 4.1938e-09, 3.6868e-09,  ..., 5.1909e-09, 2.9888e-09,\n",
      "         4.5822e-09],\n",
      "        [9.7862e-09, 2.5608e-09, 3.5461e-09,  ..., 7.2576e-09, 8.4787e-09,\n",
      "         2.7421e-09],\n",
      "        [5.2870e-09, 4.8183e-09, 6.4927e-09,  ..., 3.0546e-09, 5.6728e-09,\n",
      "         6.9275e-09]], device='cuda:0')}, 2472662805696: {'step': 29000, 'exp_avg': tensor([ 2.1521e-03,  1.6452e-11,  1.9880e-11,  ..., -3.2066e-06,\n",
      "         1.7704e-07,  5.4956e-05], device='cuda:0'), 'exp_avg_sq': tensor([1.6724e-05, 7.0447e-22, 1.2856e-21,  ..., 6.3136e-09, 1.0956e-08,\n",
      "        7.5867e-09], device='cuda:0')}}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [2473065133312, 2472885619008, 2472885603840, 2472662862336, 2472662862208, 2473083832384, 2473065124608, 2473084228416, 2473084227712, 2473084230848, 2473084228992, 2473084227968, 2473084228096, 2473084229376, 2473084229440, 2473084230144, 2472662805248, 2472662804928, 2472662805696]}]\n"
     ]
    }
   ],
   "source": [
    "# getting information on trained model\n",
    "print('Model state_dict:')\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, '\\t', model.state_dict()[param_tensor].size())\n",
    "print('Optimiser state_dict:')\n",
    "for var_name in optimiser.state_dict():\n",
    "    print(var_name, '\\t', optimiser.state_dict()[var_name])\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), 'german-english-model_50_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading model\n",
    "model.load_state_dict(torch.load('german-english-model_50_epochs.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evalutation batches: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:08,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 11.258587695805517\n"
     ]
    }
   ],
   "source": [
    "def evaluate_bleu_score(model, iterator, force_stop=False):\n",
    "    model.eval()\n",
    "    print(f'Number of evalutation batches: {len(iterator)}')\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in tqdm(enumerate(iterator)):\n",
    "            german = batch.src\n",
    "            english = batch.trg\n",
    "            \n",
    "            outputs, _ = model.translate(german)\n",
    "            \n",
    "            # model predictions\n",
    "            prediction = []\n",
    "            for i in range(outputs.shape[1]):\n",
    "                sequence = outputs[:, i]\n",
    "                words = []\n",
    "                for token_idx in sequence:\n",
    "                    token = english_text.vocab.itos[int(token_idx)]\n",
    "                    if token == '<eos>' or token == '<pad>':\n",
    "                        break\n",
    "                    else:\n",
    "                        words.append(token)\n",
    "                words = ' '.join(words)\n",
    "                prediction.append(words)\n",
    "            predictions += prediction\n",
    "            \n",
    "            # human translation\n",
    "            ground_truth = []\n",
    "            for i in range(english.shape[1]):\n",
    "                sequence = english[:, i]\n",
    "                words = []\n",
    "                for token_idx in sequence:\n",
    "                    token = english_text.vocab.itos[int(token_idx)]\n",
    "#                     print(token)\n",
    "                    if token == '<eos>' or token == '<pad>':\n",
    "                        break\n",
    "                    else:\n",
    "                        words.append(token)\n",
    "                words = ' '.join(words)\n",
    "                ground_truth.append(words) \n",
    "            ground_truths += ground_truth\n",
    "            \n",
    "            if force_stop:\n",
    "                break\n",
    "    ground_truths = [ground_truths]\n",
    "#     print(predictions)\n",
    "#     print(ground_truths)\n",
    "    score = sacrebleu.corpus_bleu(predictions, ground_truths, force=True).score\n",
    "    print('BLEU score:', score)\n",
    "evaluate_bleu_score(model, valid_iterator, force_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model for actual translation\n",
    "def translate(model, text):\n",
    "    \n",
    "    tokens = german_text.preprocess(text)\n",
    "    input_tensor = german_text.process([tokens]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs, attention = model.translate(input_tensor)\n",
    "        output_tensor = outputs.squeeze(1)\n",
    "        \n",
    "        for position, token_idx in enumerate(output_tensor):\n",
    "            token = english_text.vocab.itos[int(token_idx)]\n",
    "            if token == '<eos>' or token == '<pad>':\n",
    "                break\n",
    "        eos_position = position\n",
    "        \n",
    "        translated = []\n",
    "        for i in range(outputs.shape[1]):\n",
    "            sequence = outputs[:,i]\n",
    "            words = []\n",
    "            for tok_idx in sequence:\n",
    "                tok_idx = int(tok_idx)\n",
    "                token = english_text.vocab.itos[tok_idx]\n",
    "                if token == '<eos>' or token == '<pad>':\n",
    "                    break\n",
    "                else:\n",
    "                    words.append(token)\n",
    "            words = \" \".join(words)\n",
    "            translated.append(words)\n",
    "    print(f'Translating English: \"{text}\" to German: \"{translated[0][5:]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating English: \"der mann trinkt bier.\" to German: \" the man is drinking beer .\"\n"
     ]
    }
   ],
   "source": [
    "translate(model, 'der mann trinkt bier.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating English: \"die frau isst apfel.\" to German: \" the woman is eating .\"\n"
     ]
    }
   ],
   "source": [
    "translate(model, 'die frau isst apfel.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating English: \"die jungen spielen mit einer katze.\" to German: \" the boys are playing with a cat .\"\n"
     ]
    }
   ],
   "source": [
    "translate(model, 'die jungen spielen mit einer katze.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating English: \"sie kommt aus deutschland.\" to German: \" it is coming out of <unk> .\"\n"
     ]
    }
   ],
   "source": [
    "translate(model, 'sie kommt aus deutschland.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
